{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e5558d7-112a-4bbd-a443-f934ca34f413",
   "metadata": {},
   "source": [
    "<h1>MSIN0166: Data Engineering - Individual Cousework</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c563bead-6427-42ac-87e5-7d4b43db4ced",
   "metadata": {},
   "source": [
    "<p>Project Title: RAG-based AI Policy, User Sentiment and LLM Information Assistant API<br/>\n",
    "Notebook Author: Tuhan Sapumanage - 24223873</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b067788-9492-4e94-beb6-91f08709fd59",
   "metadata": {},
   "source": [
    "<b>Overview</b>\n",
    "- Ingests data from multiple sources: Web scraping/API+MongoDB (Hacker News Stories & Comments), PDFs (EU AI Act), CSVs (benchmarks)\n",
    "- Handles both structured and unstructured data formats, including Parquet, CSV, PDF, and raw JSON.\n",
    "- Implements preprocessing pipelines for cleaning, deduplication, chunking, and anonymisation of data to prepare it for LLM input.\n",
    "- Ensures data privacy and sensitivity handling, including HTML decoding and PII anonymisation (emails, usernames, URLs).\n",
    "- Uses secure handling of secrets via .env and GitHub Codespaces secrets to avoid hardcoded credentials.\n",
    "- Combines and enriches datasets using PySpark with fuzzy matching to simulate a scalable enterprise data transformation workflow.\n",
    "- Builds semantic vector indexes using Chroma and FAISS for scalable Retrieval-Augmented Generation (RAG).\n",
    "- Deploys a full RAG-based LLM assistant using Langchain and Gemini API, with prompt chaining and strict citation instructions to reduce hallucinations.\n",
    "- Implements prompt versioning, hallucination detection, and logging of similarity scores to Neptune for LLMOps-style observability.\n",
    "- Tracks complete data lineage using W3C PROV standard, capturing all stages from ingestion to model response.\n",
    "- Provides a modular, reproducible, and Docker-compatible pipeline structure, with environment and dependency isolation.\n",
    "- Follows enterprise best practices across MLOps/LLMOps including structured codebase, observability, provenance, and automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbff8089-2be4-4d41-86aa-d0903a1bf7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a61a5d5-8631-4a00-b2ee-230b9cf87be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c238ffd-c5af-4375-bc1e-101ed9f757d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install prov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ab8d5-d26c-4540-b098-d1af0b4d9a37",
   "metadata": {},
   "source": [
    "## 1. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5830f07-935e-43ec-9aa4-060848588c13",
   "metadata": {},
   "source": [
    "### 1.1 Loading Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e706c1cb-7cd5-4d48-a7f8-38dc257bee1f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Note:</strong> Environment variables are stored securely in the GitHub repository under Codespaces secrets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c4e5bc-f698-42cd-9db6-2343dd9e17f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594679ac-e04c-4c29-8fa4-4af97084ef3f",
   "metadata": {},
   "source": [
    "### 1.2 W3C PROV Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c16475-6735-4576-a403-21b89dffd9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prov.model import ProvDocument, ProvAgent, ProvEntity, ProvActivity\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Global prov document\n",
    "prov_doc = ProvDocument()\n",
    "prov_doc.add_namespace('ex', 'http://example.org')\n",
    "\n",
    "def record_provenance_step(entity_name: str, entity_attrs: dict, activity_name: str, agent_name: str):\n",
    "    # Unique IDs\n",
    "    eid = f\"ex:{entity_name}_{uuid.uuid4().hex[:6]}\"\n",
    "    aid = f\"ex:{activity_name}_{uuid.uuid4().hex[:6]}\"\n",
    "    agid = f\"ex:{agent_name}\"\n",
    "\n",
    "    # Register items\n",
    "    agent = prov_doc.agent(agid, {\"prov:type\": \"prov:SoftwareAgent\"})\n",
    "    entity = prov_doc.entity(eid, entity_attrs)\n",
    "    activity = prov_doc.activity(aid, startTime=datetime.now())\n",
    "\n",
    "    # Link relations\n",
    "    prov_doc.wasAssociatedWith(activity, agent)\n",
    "    prov_doc.used(activity, eid)\n",
    "    prov_doc.wasGeneratedBy(eid, activity)\n",
    "    prov_doc.wasAttributedTo(eid, agid)\n",
    "\n",
    "    return eid, aid, agid\n",
    "\n",
    "def export_provenance(path=\"data_lineage.json\"):\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(prov_doc.serialize(format=\"json\"))\n",
    "    print(f\"Provenance exported to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7699d26-4a79-4a81-b964-bbb1565501b4",
   "metadata": {},
   "source": [
    "### 1.3 MongoDB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7d4773-9e5a-40fb-a8f4-0c4c1a6b5e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "uri = \"mongodb+srv://ucl-de2:\"+os.getenv('MONGODB_PASSWORD')+\"@ucl-de2.vhnqkif.mongodb.net/?retryWrites=true&w=majority&appName=ucl-de2\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "db = client['ucl-de2']\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e749bf-a740-416a-9a4b-9adcffcf9185",
   "metadata": {},
   "source": [
    "## 2. Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5d6d2-a28d-407c-9323-4ba618f24196",
   "metadata": {},
   "source": [
    "### 2.1 Reusable Function to Save to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b41a0db5-8d27-444e-b1b5-38ec94f8a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import UpdateOne\n",
    "\n",
    "def bulk_upsert_df_to_mongo(df, collection_name):\n",
    "    \"\"\"\n",
    "    Bulk upserts DataFrame rows into the MongoDB collection (by name) using 'objectID' as the unique key.\n",
    "    Includes try-except for error handling.\n",
    "    \"\"\"\n",
    "    collection = db[collection_name]  # assumes global `db` is set\n",
    "\n",
    "    operations = []\n",
    "    index_to_objectID = {}\n",
    "\n",
    "    for i, row in enumerate(df.itertuples(index=False)):\n",
    "        doc = row._asdict()\n",
    "        object_id = doc.get(\"objectID\")\n",
    "\n",
    "        if object_id is None:\n",
    "            continue  # Skip rows without 'objectID'\n",
    "\n",
    "        operations.append(\n",
    "            UpdateOne(\n",
    "                {\"objectID\": object_id},\n",
    "                {\"$setOnInsert\": doc},\n",
    "                upsert=True\n",
    "            )\n",
    "        )\n",
    "        index_to_objectID[i] = object_id\n",
    "\n",
    "    try:\n",
    "        if operations:\n",
    "            result = collection.bulk_write(operations)\n",
    "            inserted_ids = result.upserted_ids.keys()\n",
    "            inserted_objectIDs = [index_to_objectID[i] for i in inserted_ids]\n",
    "        else:\n",
    "            print(\"No operations performed (empty or invalid input).\")\n",
    "    except Exception as e:\n",
    "        print(f\"[MongoDB Error] Failed to upsert into '{collection_name}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f6a7f-71e6-4a64-9471-2a5903fe535e",
   "metadata": {},
   "source": [
    "### 2.2 Web-scraping & Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50f277e2-f9fe-4963-9cec-8d85d36cd6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/m4kct0cj1vg9z56yyrqmggdr0000gn/T/ipykernel_77747/3010361392.py:16: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  since_timestamp = int((datetime.utcnow() - timedelta(days=DAYS_BACK)).timestamp())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching recent items for: 'AI'\n",
      "Fetching recent items for: 'GPT'\n",
      "Fetching recent items for: 'LLM'\n",
      "Fetching recent items for: 'ChatGPT'\n",
      "Fetching recent items for: 'OpenAI'\n",
      "Fetching recent items for: 'Anthropic'\n",
      "Saved 229 exact-match stories.\n",
      "Saved 417 exact-match comments.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === Config ===\n",
    "KEYWORDS = [\"AI\", \"GPT\", \"LLM\", \"ChatGPT\", \"OpenAI\", \"Anthropic\"]\n",
    "DAYS_BACK = 7\n",
    "STORY_LIMIT = 100\n",
    "COMMENT_LIMIT = 100\n",
    "STORY_FILE = \"ai_stories.parquet\"\n",
    "COMMENT_FILE = \"ai_comments.parquet\"\n",
    "\n",
    "# === Compute timestamp for past N days\n",
    "since_timestamp = int((datetime.utcnow() - timedelta(days=DAYS_BACK)).timestamp())\n",
    "\n",
    "# === Fetch from API\n",
    "def fetch_hn_data(query, tags, hits_per_page):\n",
    "    try:\n",
    "        url = \"https://hn.algolia.com/api/v1/search\"\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"tags\": tags,\n",
    "            \"hitsPerPage\": hits_per_page,\n",
    "            \"numericFilters\": f\"created_at_i>{since_timestamp}\"\n",
    "        }\n",
    "        res = requests.get(url, params=params)\n",
    "        res.raise_for_status()\n",
    "        return res.json()[\"hits\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[HN API Error] Failed for query='{query}', tags='{tags}': {e}\")\n",
    "        return []\n",
    "\n",
    "# === Exact keyword match function\n",
    "# Checks if a keyword exactly matches a word boundary in the text (not just substring)\n",
    "def contains_exact_keyword(text, keyword):\n",
    "    if not text:\n",
    "        return False\n",
    "    return re.search(rf\"\\b{re.escape(keyword)}\\b\", text, re.IGNORECASE) is not None\n",
    "\n",
    "# === Format + filter stories\n",
    "def format_stories(raw_hits, keyword):\n",
    "    filtered = []\n",
    "    for h in raw_hits:\n",
    "        if contains_exact_keyword(h.get(\"title\", \"\"), keyword):\n",
    "            filtered.append({\n",
    "                \"objectID\": h[\"objectID\"],\n",
    "                \"title\": h.get(\"title\"),\n",
    "                \"url\": h.get(\"url\"),\n",
    "                \"author\": h.get(\"author\"),\n",
    "                \"points\": h.get(\"points\"),\n",
    "                \"created_at\": h.get(\"created_at\"),\n",
    "                \"story_text\": h.get(\"story_text\"),\n",
    "                \"matched_keyword\": keyword\n",
    "            })\n",
    "    return filtered\n",
    "\n",
    "# === Format + filter comments\n",
    "def format_comments(raw_hits, keyword):\n",
    "    filtered = []\n",
    "    for h in raw_hits:\n",
    "        if contains_exact_keyword(h.get(\"comment_text\", \"\"), keyword):\n",
    "            filtered.append({\n",
    "                \"objectID\": h[\"objectID\"],\n",
    "                \"comment_text\": h.get(\"comment_text\"),\n",
    "                \"author\": h.get(\"author\"),\n",
    "                \"story_title\": h.get(\"story_title\"),\n",
    "                \"story_id\": h.get(\"story_id\"),\n",
    "                \"created_at\": h.get(\"created_at\"),\n",
    "                \"matched_keyword\": keyword\n",
    "            })\n",
    "    return filtered\n",
    "\n",
    "# === Run\n",
    "if __name__ == \"__main__\":\n",
    "    all_stories, all_comments = [], []\n",
    "\n",
    "    # Fetch stories and comments per keyword from Hacker News API\n",
    "    for kw in KEYWORDS:\n",
    "        print(f\"Fetching recent items for: '{kw}'\")\n",
    "        raw_stories = fetch_hn_data(kw, \"story\", STORY_LIMIT)\n",
    "        raw_comments = fetch_hn_data(kw, \"comment\", COMMENT_LIMIT)\n",
    "\n",
    "        all_stories += format_stories(raw_stories, kw)\n",
    "        all_comments += format_comments(raw_comments, kw)\n",
    "\n",
    "    # === Convert filtered story/comment lists into DataFrames and remove duplicates\n",
    "    story_df = pd.DataFrame(all_stories).drop_duplicates(subset=\"objectID\")\n",
    "    comment_df = pd.DataFrame(all_comments).drop_duplicates(subset=\"objectID\")\n",
    "\n",
    "\n",
    "    bulk_upsert_df_to_mongo(story_df, \"hn_stories\")\n",
    "    bulk_upsert_df_to_mongo(comment_df, \"hn_comments\")\n",
    "\n",
    "    print(f\"Saved {len(story_df)} exact-match stories.\")\n",
    "    print(f\"Saved {len(comment_df)} exact-match comments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49f60610-ab3d-4f4f-b824-5f44824035ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ex:hn_comments_raw_12ba3b',\n",
       " 'ex:hn_scrape_comments_680cfe',\n",
       " 'ex:hn_scraper_script')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging to W3C PROV\n",
    "record_provenance_step(\n",
    "    entity_name=\"hn_stories_raw\",\n",
    "    entity_attrs={\"prov:label\": \"Raw HN stories\", \"prov:type\": \"prov:Entity\", \"ex:source\": \"hn.algolia.com\"},\n",
    "    activity_name=\"hn_scrape_stories\",\n",
    "    agent_name=\"hn_scraper_script\"\n",
    ")\n",
    "\n",
    "record_provenance_step(\n",
    "    entity_name=\"hn_comments_raw\",\n",
    "    entity_attrs={\"prov:label\": \"Raw HN comments\", \"prov:type\": \"prov:Entity\", \"ex:source\": \"hn.algolia.com\"},\n",
    "    activity_name=\"hn_scrape_comments\",\n",
    "    agent_name=\"hn_scraper_script\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c04e88-d390-4a30-bbad-0f26d6305103",
   "metadata": {},
   "source": [
    "#### 2.2.1 Chunking Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f001a-1183-445f-8ce9-bc4da0193f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"chunks\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e71a38c-4b8e-4c9d-8c5c-8995e109fe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 522 story chunks and 1439 comment chunks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Config ===\n",
    "CHUNK_SIZE = 500\n",
    "OVERLAP = 100\n",
    "\n",
    "# === Split long text into overlapping chunks to support semantic search\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# === Load stories from MongoDB\n",
    "df_stories = pd.DataFrame(list(db[\"hn_stories\"].find()))\n",
    "story_chunks = []\n",
    "\n",
    "# === Chunk Hacker News stories using story_text or title\n",
    "for _, row in df_stories.iterrows():\n",
    "    text = row.get(\"story_text\") or row.get(\"title\") or \"\"\n",
    "    if text and isinstance(text, str):\n",
    "        chunks = chunk_text(text, CHUNK_SIZE, OVERLAP)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            story_chunks.append({\n",
    "                \"source\": \"HackerNews_Story\",\n",
    "                \"chunk_id\": f\"{row['objectID']}_{i}\",\n",
    "                \"story_id\": row['objectID'],\n",
    "                \"text\": chunk\n",
    "            })\n",
    "\n",
    "df_story_chunks = pd.DataFrame(story_chunks)\n",
    "\n",
    "# === Load comments from MongoDB\n",
    "df_comments = pd.DataFrame(list(db[\"hn_comments\"].find()))\n",
    "df_comments = df_comments[[\"objectID\", \"comment_text\", \"story_id\"]].dropna(subset=[\"comment_text\"])\n",
    "\n",
    "df_comment_chunks = pd.DataFrame({\n",
    "    \"source\": \"HackerNews_Comment\",\n",
    "    \"chunk_id\": df_comments[\"objectID\"].astype(str),\n",
    "    \"story_id\": df_comments.get(\"story_id\", None),\n",
    "    \"text\": df_comments[\"comment_text\"]\n",
    "})\n",
    "\n",
    "# === Save as parquet\n",
    "df_story_chunks.to_parquet(\"chunks/hn_story_chunks.parquet\", index=False)\n",
    "df_comment_chunks.to_parquet(\"chunks/hn_comment_chunks.parquet\", index=False)\n",
    "\n",
    "print(f\"Saved {len(df_story_chunks)} story chunks and {len(df_comment_chunks)} comment chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ffe037b-6fb1-4817-9343-47a431c240de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pymupdf pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59ab85-42b2-4ec0-975c-1ae09c320965",
   "metadata": {},
   "source": [
    "### 2.3 Chunking EU AI Act PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb363d24-a1db-4cda-ab3b-ada68e63d196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from EU AI Act PDF...\n",
      "Splitting into chunks...\n",
      "Saved 750 chunks to chunks/eu_ai_act_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "PDF_FILE = \"data/eu_ai_act.pdf\"\n",
    "CHUNK_SIZE = 1000  # characters\n",
    "CHUNK_OVERLAP = 200\n",
    "PARQUET_OUT = \"chunks/eu_ai_act_chunks.parquet\"\n",
    "\n",
    "# === Extract all text from the PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    return full_text\n",
    "\n",
    "# === Split text into overlapping chunks\n",
    "def split_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# === Run the extraction + save\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"Extracting text from EU AI Act PDF...\")\n",
    "        full_text = extract_text_from_pdf(PDF_FILE)\n",
    "\n",
    "        print(\"Splitting into chunks...\")\n",
    "        chunks = split_into_chunks(full_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "        df_chunks = pd.DataFrame({\n",
    "            \"source\": \"EU_AI_Act\",\n",
    "            \"chunk_id\": range(len(chunks)),\n",
    "            \"text\": chunks\n",
    "        })\n",
    "\n",
    "        df_chunks.to_parquet(PARQUET_OUT, index=False)\n",
    "        print(f\"Saved {len(df_chunks)} chunks to {PARQUET_OUT}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PDF Error] Could not process {PDF_FILE}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b83589e-d5eb-4e1f-b614-82bda548f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging to W3C PROV\n",
    "eid, aid, agid = record_provenance_step(\n",
    "    entity_name=\"eu_ai_act_chunks\",\n",
    "    entity_attrs={\"prov:label\": \"Chunks from EU AI Act\", \"prov:type\": \"prov:Entity\"},\n",
    "    activity_name=\"pdf_chunking\",\n",
    "    agent_name=\"notebook_script\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10eb9bf6-b031-4319-bb5d-0890972b0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e16904-093e-4b77-b2ff-f56d4c39711e",
   "metadata": {},
   "source": [
    "### 2.4 Benchmark & Transparency CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7c2e8-a14c-4c8d-b87b-8e70ecc368b2",
   "metadata": {},
   "source": [
    "#### 2.4.1 PySpark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40de4f06-7eea-4567-8c10-cd18838bf8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/07 14:21:52 WARN Utils: Your hostname, Ts-MacBook.local resolves to a loopback address: 127.0.0.1; using 10.52.141.81 instead (on interface en0)\n",
      "25/04/07 14:21:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/07 14:21:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, regexp_replace, col\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AISpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read two datasets\n",
    "df1 = spark.read.csv(\"data/llm_benchmarks.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"data/ai_transparency.csv\", header=True, inferSchema=True)\n",
    "df1 = df1.withColumnRenamed(\"model\", \"modelsource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "768d25bc-5805-4d0f-90c2-3591f44d7ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ex:ai_transparency_csv_68b382',\n",
       " 'ex:csv_import_transparency_cbfc66',\n",
       " 'ex:spark_loader')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging to W3C PROV\n",
    "record_provenance_step(\n",
    "    entity_name=\"llm_benchmarks_csv\",\n",
    "    entity_attrs={\"prov:label\": \"LLM benchmark CSV\", \"prov:type\": \"prov:Entity\", \"ex:path\": \"data/llm_benchmarks.csv\"},\n",
    "    activity_name=\"csv_import_llm_benchmark\",\n",
    "    agent_name=\"spark_loader\"\n",
    ")\n",
    "\n",
    "record_provenance_step(\n",
    "    entity_name=\"ai_transparency_csv\",\n",
    "    entity_attrs={\"prov:label\": \"AI transparency CSV\", \"prov:type\": \"prov:Entity\", \"ex:path\": \"data/ai_transparency.csv\"},\n",
    "    activity_name=\"csv_import_transparency\",\n",
    "    agent_name=\"spark_loader\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ee143-ea4f-41e0-adce-2da63dddcf16",
   "metadata": {},
   "source": [
    "#### 2.4.2 PySpark Fuzzy Combine & Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bbc7028-055d-4f81-bd2e-644701245660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/07 14:21:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to llm_benchs_transparency_temp.parquet\n",
      "Saved as single file: llm_benchs_transparency_chunks.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, col, levenshtein\n",
    "\n",
    "# Step 0: Ensure 'Type' and 'Model' columns are strings\n",
    "df1 = df1.withColumn(\"Type\", col(\"Type\").cast(\"string\"))\n",
    "df2 = df2.withColumn(\"Model\", col(\"Model\").cast(\"string\"))\n",
    "\n",
    "# Step 1: Clean both sides (remove non-alphanum, lowercase\n",
    "# === Clean and standardize strings to improve fuzzy matching accuracy\n",
    "df1_clean = df1.withColumn(\"Type_clean\", regexp_replace(lower(col(\"Type\")), \"[^a-z0-9]\", \"\"))\n",
    "df2_clean = df2.withColumn(\"Model_clean\", regexp_replace(lower(col(\"Model\")), \"[^a-z0-9]\", \"\"))\n",
    "\n",
    "# Step 2: Cross join and fuzzy match\n",
    "# === Match similar model names using Levenshtein distance and partial string matching\n",
    "try:\n",
    "    joined = df1_clean.crossJoin(df2_clean).filter(\n",
    "        (levenshtein(col(\"Type_clean\"), col(\"Model_clean\")) <= 3) |\n",
    "        (col(\"Type_clean\").contains(col(\"Model_clean\"))) |\n",
    "        (col(\"Model_clean\").contains(col(\"Type_clean\")))\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"[Spark Error] During fuzzy join: {e}\")\n",
    "\n",
    "# Step 3: Drop helper columns if desired\n",
    "result = joined.drop(\"Type_clean\", \"Model_clean\")\n",
    "\n",
    "from pyspark.sql.functions import col, concat_ws, lit, coalesce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Dynamically build \"Key: Value\" lines for each column\n",
    "# === Combine all attributes into one textual \"chunk\" per matched row\n",
    "text_lines = [\n",
    "    F.concat(F.lit(f\"{c}: \"), coalesce(col(c).cast(\"string\"), lit(\"N/A\")))\n",
    "    for c in result.columns\n",
    "]\n",
    "\n",
    "# Step 2: Combine all lines into one multi-line text chunk per row\n",
    "result_with_text = result.withColumn(\"text\", concat_ws(\"\\n\", *text_lines))\n",
    "\n",
    "# Step 3: Add metadata columns\n",
    "result_with_chunks = result_with_text.withColumn(\"source\", lit(\"LLM_Benches_Transparency\")) \\\n",
    "                                     .withColumn(\"chunk_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Step 4: Keep only the final columns\n",
    "final_chunks = result_with_chunks.select(\"source\", \"chunk_id\", \"text\")\n",
    "\n",
    "# Step 5: Save as Parquet file\n",
    "final_chunks.write.mode(\"overwrite\").parquet(\"llm_benchs_transparency_temp.parquet\")\n",
    "\n",
    "print(\"Saved chunks to llm_benchs_transparency_temp.parquet\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Directory Spark wrote to\n",
    "output_dir = \"llm_benchs_transparency_temp.parquet\"\n",
    "\n",
    "# Find the actual part file\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.startswith(\"part-\") and filename.endswith(\".parquet\"):\n",
    "        shutil.move(os.path.join(output_dir, filename), \"chunks/llm_benchs_transparency_chunks.parquet\")\n",
    "\n",
    "# (Optional) Remove the original output directory\n",
    "shutil.rmtree(output_dir)\n",
    "\n",
    "print(\"Saved as single file: llm_benchs_transparency_chunks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "091fe02d-b984-4c52-9cb9-ec575f65bcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ex:llm_transparency_merged_b2d127',\n",
       " 'ex:fuzzy_matching_merge_618f12',\n",
       " 'ex:spark_merger')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging to W3C PROV\n",
    "record_provenance_step(\n",
    "    entity_name=\"llm_transparency_merged\",\n",
    "    entity_attrs={\"prov:label\": \"Fuzzy-matched LLM x Transparency\", \"prov:type\": \"prov:Entity\"},\n",
    "    activity_name=\"fuzzy_matching_merge\",\n",
    "    agent_name=\"spark_merger\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc48a36-86a4-4bd5-a291-63b284a27e30",
   "metadata": {},
   "source": [
    "## 3. Storing in Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551eff8-148c-4dec-a7b1-12c2eac03913",
   "metadata": {},
   "source": [
    "### 3.1 Handling PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "611ccbc3-ec88-4e31-b444-a6d438199ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "import html\n",
    "\n",
    "# Store original vs anonymised texts for hn_comment\n",
    "anonymised_examples = []\n",
    "\n",
    "# === Replace personal details like usernames, emails, and links with placeholders\n",
    "def anonymise_comment(text):\n",
    "    # Step 1: Decode HTML entities\n",
    "    decoded = html.unescape(text)\n",
    "\n",
    "    # Step 2: Anonymise patterns\n",
    "    decoded = re.sub(r'\\bby\\s+\\w+', 'by [user]', decoded, flags=re.IGNORECASE)\n",
    "    decoded = re.sub(r'@\\w+', '@[user]', decoded)\n",
    "    decoded = re.sub(r'\\b[\\w\\.-]+\\s*\\[\\s*at\\s*\\]\\s*[\\w\\.-]+\\.\\w+\\b', '[email]', decoded, flags=re.IGNORECASE)  # nonstandard email\n",
    "    decoded = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '[email]', decoded)  # normal email\n",
    "    decoded = re.sub(r'https?://\\S+', '[link]', decoded)  # raw URLs\n",
    "    decoded = re.sub(r'rel=\"nofollow\">[^<]+</a>', 'rel=\"nofollow\">[link]</a>', decoded)  # HTML links\n",
    "\n",
    "    # Step 3: Optionally strip tags like <p>, <br>\n",
    "    decoded = re.sub(r'</?p>', '', decoded, flags=re.IGNORECASE)\n",
    "    decoded = re.sub(r'</?br\\s*/?>', '', decoded, flags=re.IGNORECASE)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "def load_documents_from_parquet(parquet_path: str, source_name: str = \"\") -> list:\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    if 'text' not in df.columns:\n",
    "        raise ValueError(f\"'text' column not found in {parquet_path}\")\n",
    "\n",
    "    if 'metadata' not in df.columns:\n",
    "        df['metadata'] = [{} for _ in range(len(df))]\n",
    "\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        if source_name == \"hn_comment\":\n",
    "            text_anon = anonymise_comment(text)\n",
    "            # === Collect original vs anonymised examples for quality inspection\n",
    "            anonymised_examples.append((text, text_anon))\n",
    "            text = text_anon\n",
    "\n",
    "        documents.append(Document(\n",
    "            page_content=text,\n",
    "            metadata=row['metadata'] if isinstance(row['metadata'], dict) else {}\n",
    "        ))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b3f65-2960-4078-9d43-d74b899ea015",
   "metadata": {},
   "source": [
    "### 3.2 Saving to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b65c255b-82c8-4f68-91a3-687e118942c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/m4kct0cj1vg9z56yyrqmggdr0000gn/T/ipykernel_77747/1191640227.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: chunks/eu_ai_act_chunks.parquet\n",
      "Building vector store: store/eu_ai_act_db (750 documents)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/m4kct0cj1vg9z56yyrqmggdr0000gn/T/ipykernel_77747/1191640227.py:28: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to store/eu_ai_act_db\n",
      "\n",
      "Processing: chunks/hn_story_chunks.parquet\n",
      "Building vector store: store/hn_story_db (522 documents)\n",
      "Saved to store/hn_story_db\n",
      "\n",
      "Processing: chunks/hn_comment_chunks.parquet\n",
      "Building vector store: store/hn_comment_db (1439 documents)\n",
      "Saved to store/hn_comment_db\n",
      "\n",
      "Processing: chunks/llm_benchs_transparency_chunks.parquet\n",
      "Building vector store: store/llm_benchs_transparency_db (26915 documents)\n",
      "Saved to store/llm_benchs_transparency_db\n",
      "\n",
      "\n",
      "--- Sample Anonymised HN Comments ---\n",
      "\n",
      "Example 1\n",
      "Original:\n",
      " For web UI we have Fable and it&#x27;s integration with well-known js frameworks, also WebSharper (although it&#x27;s less well-known) and Bolero (on top of Blazor)<p>For mobile we have FuncUI (on top of Avalonia) and Fabulous (on top of Avalonia, Xamarin and Maui).\n",
      "Most of these frameworks use Elm architecture, but some do not. For example I use Oxpecker.Solid which has reactive architecture.<p>Can&#x27;t help with Scala comparison, but at least DeepSeek V3 prefers F# for UI <a href=\"https:&#x2F;&#x2F;whatisbetter.ai&#x2F;result&#x2F;F%23-vs-Scala-9eaede00c7e448568f6756095849ae2b\" rel=\"nofollow\">https:&#x2F;&#x2F;whatisbetter.ai&#x2F;result&#x2F;F%23-vs-Scala-9eaede00c7e4485...</a>\n",
      "Anonymised:\n",
      " For web UI we have Fable and it's integration with well-known js frameworks, also WebSharper (although it's less well-known) and Bolero (on top of Blazor)For mobile we have FuncUI (on top of Avalonia) and Fabulous (on top of Avalonia, Xamarin and Maui).\n",
      "Most of these frameworks use Elm architecture, but some do not. For example I use Oxpecker.Solid which has reactive architecture.Can't help with Scala comparison, but at least DeepSeek V3 prefers F# for UI <a href=\"[link] rel=\"nofollow\">[link]\n",
      "\n",
      "Example 2\n",
      "Original:\n",
      " I don&#x27;t think pretend AI is still a trade secret.\n",
      "Anonymised:\n",
      " I don't think pretend AI is still a trade secret.\n",
      "\n",
      "Example 3\n",
      "Original:\n",
      " The important question here isn&#x27;t who owns the copyright in the output, but rather whether the output also infringes on somebody else&#x27;s, and if so, who&#x27;s liable for that: The company that trained the AI, the one that hosts it as a service, or the user that prompted it.\n",
      "Anonymised:\n",
      " The important question here isn't who owns the copyright in the output, but rather whether the output also infringes on somebody else's, and if so, who's liable for that: The company that trained the AI, the one that hosts it as a service, or the user that prompted it.\n",
      "\n",
      "Example 4\n",
      "Original:\n",
      " Location: Brazil<p>Remote: Yes<p>Willing to relocate: Yes<p>Technologies: .NET&#x2F;C#&#x2F;ASP.NET Core, Java&#x2F;Spring Boot, Python&#x2F;Django&#x2F;FastAPI, PostgreSQL&#x2F;SQL Server&#x2F;MySQL, MongoDB &#x2F; CosmosDB &#x2F; Dynamo, React, Angular, AWS&#x2F;Azure, Docker, Kubernetes&#x2F;OpenShift, Selenium, Cypress, K6, Gatling, Jenkins, DevSecOps, AI&#x2F;ML.<p>LinkedIn: <a href=\"https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;felipemcoelho\" rel=\"nofollow\">https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;felipemcoelho</a><p>Email: felipemc [at] live.com<p>Developer with 6+ years of experience, primarily with backend development, using .NET, Java and Python. Degree in Cyber Security. QA and test automation background. Experienced with AWS and Azure, DevSecOps and AI&#x2F;ML.<p>Quick learner and fast problem-solver. Open to learning new stacks. If I don’t know something, I’m dedicated to learning it quickly and delivering results. Motivated to grow and contribute.<p>Flexible with remote&#x2F;hybrid&#x2F;onsite work and open to relocation.\n",
      "Anonymised:\n",
      " Location: BrazilRemote: YesWilling to relocate: YesTechnologies: .NET/C#/ASP.NET Core, Java/Spring Boot, Python/Django/FastAPI, PostgreSQL/SQL Server/MySQL, MongoDB / CosmosDB / Dynamo, React, Angular, AWS/Azure, Docker, Kubernetes/OpenShift, Selenium, Cypress, K6, Gatling, Jenkins, DevSecOps, AI/ML.LinkedIn: <a href=\"[link] rel=\"nofollow\">[link] [email]Developer with 6+ years of experience, primarily with backend development, using .NET, Java and Python. Degree in Cyber Security. QA and test automation background. Experienced with AWS and Azure, DevSecOps and AI/ML.Quick learner and fast problem-solver. Open to learning new stacks. If I don’t know something, I’m dedicated to learning it quickly and delivering results. Motivated to grow and contribute.Flexible with remote/hybrid/onsite work and open to relocation.\n",
      "\n",
      "Example 5\n",
      "Original:\n",
      " It&#x27;s good, and funny, and does a great job of simplifying financial complexity down into small morsels of understandable content.<p>Now that he has a podcast, I would like someone to generate an AI reader in the style of his voice so I can listen to it when driving.\n",
      "Anonymised:\n",
      " It's good, and funny, and does a great job of simplifying financial complexity down into small morsels of understandable content.Now that he has a podcast, I would like someone to generate an AI reader in the style of his voice so I can listen to it when driving.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "datasets = {\n",
    "    \"eu_ai_act\": \"chunks/eu_ai_act_chunks.parquet\",\n",
    "    \"hn_story\": \"chunks/hn_story_chunks.parquet\",\n",
    "    \"hn_comment\": \"chunks/hn_comment_chunks.parquet\",\n",
    "    \"llm_benchs_transparency\": \"chunks/llm_benchs_transparency_chunks.parquet\"\n",
    "}\n",
    "\n",
    "# === Save document embeddings using Chroma vector store (on-disk)\n",
    "def build_vectorstore(documents: list, persist_directory: str):\n",
    "    print(f\"Building vector store: {persist_directory} ({len(documents)} documents)\")\n",
    "    try:\n",
    "        if not os.path.exists(persist_directory):\n",
    "            os.makedirs(persist_directory)\n",
    "\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents,\n",
    "            embedding=embedding_function,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        vectorstore.persist()\n",
    "        print(f\"Saved to {persist_directory}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[VectorStore Error] Failed to build store at {persist_directory}: {e}\")\n",
    "\n",
    "# --- Run All ---\n",
    "for name, parquet_file in datasets.items():\n",
    "    db_dir = f\"store/{name}_db\"\n",
    "    print(f\"Processing: {parquet_file}\")\n",
    "    \n",
    "    try:\n",
    "        docs = load_documents_from_parquet(parquet_file, source_name=name)\n",
    "        build_vectorstore(docs, db_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {name}: {e}\")\n",
    "\n",
    "# --- Print Anonymised Examples ---\n",
    "if anonymised_examples:\n",
    "    print(\"\\n--- Sample Anonymised HN Comments ---\")\n",
    "    for i, (original, anonymised) in enumerate(anonymised_examples[:5], 1):  # Show first 5\n",
    "        print(f\"\\nExample {i}\")\n",
    "        print(\"Original:\\n\", original.strip())\n",
    "        print(\"Anonymised:\\n\", anonymised.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4714a6e2-6198-4a11-af9d-50491aa8f090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ex:llm_transparency_chunks_6ae9e3',\n",
       " 'ex:transparency_chunking_286b1d',\n",
       " 'ex:chunking_module')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging to W3C PROV\n",
    "record_provenance_step(\n",
    "    entity_name=\"hn_story_chunks\",\n",
    "    entity_attrs={\"prov:label\": \"HN story chunks\", \"prov:type\": \"prov:Entity\", \"ex:source\": \"MongoDB:hn_stories\"},\n",
    "    activity_name=\"hn_story_chunking\",\n",
    "    agent_name=\"chunking_module\"\n",
    ")\n",
    "\n",
    "record_provenance_step(\n",
    "    entity_name=\"hn_comment_chunks\",\n",
    "    entity_attrs={\"prov:label\": \"HN comment chunks\", \"prov:type\": \"prov:Entity\", \"ex:source\": \"MongoDB:hn_comments\"},\n",
    "    activity_name=\"hn_comment_chunking\",\n",
    "    agent_name=\"chunking_module\"\n",
    ")\n",
    "\n",
    "record_provenance_step(\n",
    "    entity_name=\"eu_ai_act_chunks\",\n",
    "    entity_attrs={\"prov:label\": \"EU AI Act PDF chunks\", \"prov:type\": \"prov:Entity\", \"ex:source\": \"data/eu_ai_act.pdf\"},\n",
    "    activity_name=\"pdf_chunking\",\n",
    "    agent_name=\"chunking_module\"\n",
    ")\n",
    "\n",
    "record_provenance_step(\n",
    "    entity_name=\"llm_transparency_chunks\",\n",
    "    entity_attrs={\"prov:label\": \"LLM + Transparency merged chunks\", \"prov:type\": \"ex:source:Entity\"},\n",
    "    activity_name=\"transparency_chunking\",\n",
    "    agent_name=\"chunking_module\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da99f68-656f-4cfc-a5c4-72e4596b43e6",
   "metadata": {},
   "source": [
    "### 3.3 Utility Functions to Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3de9f32a-768f-44d9-87de-48b98fd828c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in vectorstore: 8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/m4kct0cj1vg9z56yyrqmggdr0000gn/T/ipykernel_77747/712621388.py:8: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Checking if successfully saved\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Example: Check for one of the saved stores\n",
    "persist_directory = \"store/eu_ai_act_db\"\n",
    "\n",
    "# Load the vector store\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# Check the number of documents in the store\n",
    "print(f\"Number of documents in vectorstore: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c731a2cb-8a21-4a81-aea5-c658879cfb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1\n",
      "e conditions for the \n",
      "evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection \n",
      "thereof. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article \n",
      "98(2).\n",
      "7.\n",
      "Prior to requesting access t\n",
      "{}\n",
      "\n",
      "Result 2\n",
      "e conditions for the \n",
      "evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection \n",
      "thereof. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article \n",
      "98(2).\n",
      "7.\n",
      "Prior to requesting access t\n",
      "{}\n",
      "\n",
      "Result 3\n",
      "e conditions for the \n",
      "evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection \n",
      "thereof. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article \n",
      "98(2).\n",
      "7.\n",
      "Prior to requesting access t\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Querying the database\n",
    "query = \"What are the obligations in the EU AI Act?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}\")\n",
    "    print(doc.page_content[:300])  # show first 300 characters\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb452f68-9573-4605-b626-73a8b86fefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q langchain chromadb sentence-transformers requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d153cfa-8c87-4d36-8fea-f73880eccce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini's Answer:\n",
      "\n",
      "Here's a summary of what's happening in AI based on the provided context:\n",
      "\n",
      "*   **AI Regulation:** The EU AI Act is considering a threshold of floating point operations for general-purpose AI models, above which a model is presumed to have systemic risks. This threshold would be adjusted over time and supplemented with benchmarks and indicators of model capability. The AI Office would engage with experts to inform these adjustments [eu\\_ai\\_act].\n",
      "\n",
      "*   **AI Progress Debate:** There are differing opinions on the future of AI progress, with some questioning whether significant improvements will continue [hn\\_comment].\n",
      "\n",
      "*   **AI Model Benchmarks:** The `aisquared/chopt-1_3b` model (OPT type, 1.3B class) achieved a score of 38.4% on a benchmark, with a throughput of 82.0 tokens/s and peak memory usage of 4737 MB [llm\\_benchmark].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing RAG before deploying as Flask API\n",
    "# Imports\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "gemini_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}\"\n",
    "\n",
    "# Load all vector stores\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_paths = {\n",
    "    \"eu_ai_act\": \"store/eu_ai_act_db\",\n",
    "    \"hn_story\": \"store/hn_story_db\",\n",
    "    \"hn_comment\": \"store/hn_comment_db\",\n",
    "    \"llm_benchmark\": \"store/llm_benchmark_db\"\n",
    "}\n",
    "stores = {name: Chroma(persist_directory=path, embedding_function=embedding) for name, path in vector_paths.items()}\n",
    "\n",
    "# Ask your question\n",
    "query = \"What is currently happening in AI (tell me news, user opinions, benchmarks, etc)?\"\n",
    "top_k = 3\n",
    "\n",
    "# Search each vector store and collect results\n",
    "docs = []\n",
    "for name, store in stores.items():\n",
    "    results = store.similarity_search(query, k=top_k)\n",
    "    for doc in results:\n",
    "        doc.metadata[\"source\"] = name\n",
    "        docs.append(doc)\n",
    "\n",
    "# Format context\n",
    "context = \"\\n\\n\".join([f\"[{doc.metadata['source']}] {doc.page_content.strip()}\" for doc in docs])\n",
    "\n",
    "# Create Gemini prompt\n",
    "prompt = f\"\"\"\n",
    "You are an expert assistant in AI policy.\n",
    "\n",
    "Using the context provided below, answer the user's question. Cite from the text if relevant.\n",
    "Be accurate and concise. Tell me the source too in square brackets.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Send to Gemini\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "response = requests.post(gemini_url, headers=headers, json=payload)\n",
    "\n",
    "# Display answer\n",
    "if response.status_code == 200:\n",
    "    answer = response.json()['candidates'][0]['content']['parts'][0]['text']\n",
    "    print(\"Gemini's Answer:\\n\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3afdfcf7-9a74-481a-8c58-5bf154bcbd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources used in context: ['eu_ai_act', 'eu_ai_act', 'eu_ai_act', 'hn_story', 'hn_story', 'hn_story', 'hn_comment', 'hn_comment', 'hn_comment', 'llm_benchmark', 'llm_benchmark', 'llm_benchmark']\n"
     ]
    }
   ],
   "source": [
    "# Verifying sources 1\n",
    "sources_used = [doc.metadata[\"source\"] for doc in docs]\n",
    "print(\"Sources used in context:\", sources_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87a56236-13d4-4c98-9c04-7828a6f1c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in prompt: [eu_ai_act] s of the model prior to deployment, such as \n",
      "pre-training, synthetic data generation and fine-tuning. Therefore, an initial threshold of floating point operations \n",
      "should be set, which, if met by a general-purpose AI model, leads to a presumption that the model is \n",
      "a general-purpose AI model with systemic risks. This threshold should be adjusted over time to reflect technological \n",
      "and industrial changes, such as algorithmic improvements or increased hardware efficiency, and should be \n",
      "supplemented with benchmarks and indicators for model capability. To inform this, the AI Office should engage \n",
      "with the scientific community, industry, civil society and other experts. Thresholds, as well as tools and benchmarks \n",
      "for the assessment of high-impact capabilities, should be strong predictors of generality, its capabilities and \n",
      "associated systemic risk of general-purpose AI models, and could take into account the way the model will be placed \n",
      "on the market or the number of users it may affect\n",
      "\n",
      "[eu_ai_act] s of the model prior to deployment, such as \n",
      "pre-training, synthetic data generation and fine-tuning. Therefore, an initial threshold of floating point operations \n",
      "should be set, which, if met by a general-purpose AI model, leads to a presumption that the model is \n",
      "a general-purpose AI model with systemic risks. This threshold should be adjusted over time to reflect technological \n",
      "and industrial changes, such as algorithmic improvements or increased hardware efficiency, and should be \n",
      "supplemented with benchmarks and indicators for model capability. To inform this, the AI Office should engage \n",
      "with the scientific community, industry, civil society and other experts. Thresholds, as well as tools and benchmarks \n",
      "for the assessment of high-impact capabilities, should be strong predictors of generality, its capabilities and \n",
      "associated systemic risk of general-purpose AI models, and could take into account the way the model will be placed \n",
      "on the market or the number of users it may affect\n",
      "\n",
      "[eu_ai_act] s of the model prior to deployment, such as \n",
      "pre-training, synthetic data generation and fine-tuning. Therefore, an initial threshold of floating point operations \n",
      "should be set, which, if met by a general-purpose AI model, leads to a presumption that the model is \n",
      "a general-purpose AI model with systemic risks. This threshold should be adjusted over time to reflect technological \n",
      "and industrial changes, such as algorithmic improvements or increased hardware efficiency, and should be \n",
      "supplemented with benchmarks and indicators for model capability. To inform this, the AI Office should engage \n",
      "with the scientific community, industry, civil society and other experts. Thresholds, as well as tools and benchmarks \n",
      "for the assessment of high-impact capabilities, should be strong predictors of generality, its capabilities and \n",
      "associated systemic risk of general-purpose AI models, and could take into account the way the model will be placed \n",
      "on the market or the number of users it may affect\n",
      "\n",
      "[hn_story] AI Trends in the Real World\n",
      "\n",
      "[hn_story] AI Trends in the Real World\n",
      "\n",
      "[hn_story] AI Trends in the Real World\n",
      "\n",
      "[hn_comment] That seems naive in a status quo bias way to me. Why and where do you expect AI progress to stop? It sounds like somewhere very close to where we are at in your eyes. Why do you think there won't be many further improvements?\n",
      "\n",
      "[hn_comment] That seems naive in a status quo bias way to me. Why and where do you expect AI progress to stop? It sounds like somewhere very close to where we are at in your eyes. Why do you think there won't be many further improvements?\n",
      "\n",
      "[hn_comment] That seems naive in a status quo bias way to me. Why and where do you expect AI progress to stop? It sounds like somewhere very close to where we are at in your eyes. Why do you think there won't be many further improvements?\n",
      "\n",
      "[llm_benchmark] Model: aisquared/chopt-1_3b\n",
      "Type: OPT | Class: 1B\n",
      "Backend: pytorch | Dtype: float16\n",
      "Optimizations: BetterTransformer\n",
      "Throughput: 82.0 tokens/s\n",
      "Peak Memory: 4737 MB\n",
      "Score: 38.4%\n",
      "\n",
      "[llm_benchmark] Model: aisquared/chopt-1_3b\n",
      "Type: OPT | Class: 1B\n",
      "Backend: pytorch | Dtype: float16\n",
      "Optimizations: BetterTransformer\n",
      "Throughput: 82.0 tokens/s\n",
      "Peak Memory: 4737 MB\n",
      "Score: 38.4%\n",
      "\n",
      "[llm_benchmark] Model: aisquared/chopt-1_3b\n",
      "Type: OPT | Class: 1B\n",
      "Backend: pytorch | Dtype: float16\n",
      "Optimizations: BetterTransformer\n",
      "Throughput: 82.0 tokens/s\n",
      "Peak Memory: 4737 MB\n",
      "Score: 38.4%\n",
      "Total tokens in prompt: What is currently happening in AI (tell me news, user opinions, benchmarks, etc)?\n",
      "Total tokens in prompt: 1079\n"
     ]
    }
   ],
   "source": [
    "# Verifying sources 2\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the same tokenizer used for embedding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Count tokens in the prompt\n",
    "prompt_text = f\"\"\"\n",
    "You are an expert assistant in AI policy and cybersecurity.\n",
    "\n",
    "Using the context provided below, answer the user's question. Cite from the text if relevant. Be accurate and concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "token_count = len(tokenizer.encode(prompt_text))\n",
    "print(f\"Total tokens in prompt: {context}\")\n",
    "print(f\"Total tokens in prompt: {query}\")\n",
    "print(f\"Total tokens in prompt: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d598a3d9-53b8-44b8-b0f1-9305915c72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q flask python-dotenv chromadb langchain sentence-transformers requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fbcab7f6-af82-4e24-b007-31d6873d9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57dac7f7-0581-4438-b1b4-551dabe381ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ex:gemini_response_5ad319', 'ex:rag_query_answering_7634aa', 'ex:gemini_llm')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging to W3C PROV\n",
    "record_provenance_step(\n",
    "    entity_name=\"gemini_prompt_v1.3\",\n",
    "    entity_attrs={\"prov:label\": \"Gemini input prompt\", \"prov:type\": \"prov:Entity\"},\n",
    "    activity_name=\"rag_prompt_generation\",\n",
    "    agent_name=\"flask_api\"\n",
    ")\n",
    "\n",
    "record_provenance_step(\n",
    "    entity_name=\"gemini_response\",\n",
    "    entity_attrs={\"prov:label\": \"Gemini model response\", \"prov:type\": \"prov:Entity\"},\n",
    "    activity_name=\"rag_query_answering\",\n",
    "    agent_name=\"gemini_llm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c83c1-f009-494d-bcc1-71a3497fdd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"logs\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c74cb934-d493-47bd-a279-cf6d24cd937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provenance exported to logs/data_lineage.json\n"
     ]
    }
   ],
   "source": [
    "# Saving W3C PROV logs\n",
    "export_provenance(\"logs/data_lineage.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec348a-d38d-4acc-8090-bce353abca6f",
   "metadata": {},
   "source": [
    "## 4. Additional: Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "71117f96-d288-40d1-a02e-d34f0818a3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      " Here's a bullet-point summary of the EU AI Act:\n",
      "\n",
      "*   **High-Risk AI Obligations:** Includes requirements for risk management, data governance, and transparency.\n",
      "*   **Prohibited AI Practices:** Bans certain AI uses like social scoring and real-time biometric identification in public spaces (with limited exceptions).\n",
      "\n",
      "=== Analysis ===\n",
      " Okay, based on the provided summary of the EU AI Act, here's an analysis of the key risks, opportunities, and challenges:\n",
      "\n",
      "**Risks:**\n",
      "\n",
      "*   **Compliance Costs & Complexity:** The \"High-Risk AI Obligations\" related to risk management, data governance, and transparency will likely require significant investment in resources, personnel, and technology for companies developing and deploying AI within or for the EU market.  The complexity of these obligations could be a barrier to entry, especially for smaller businesses.\n",
      "*   **Legal Uncertainty:** Interpretation and implementation of the act may evolve over time, creating legal uncertainty for companies.  The specifics of \"risk management\" and \"data governance\" may require further clarification through guidelines and case law.\n",
      "*   **Reputational Risk:** Failure to comply with the AI Act could lead to significant fines and reputational damage.  The reputational damage could arise not only from penalties but also from being perceived as unethical or irresponsible in the use of AI.\n",
      "*   **Supply Chain Disruptions:** Companies relying on AI systems developed by others will need to ensure their suppliers are also compliant with the AI Act. This adds complexity and potential disruption to supply chains.\n",
      "*   **Stifling Innovation (Potential):** Overly strict or burdensome regulations could potentially stifle innovation in AI, as companies may be hesitant to pursue cutting-edge applications for fear of running afoul of the law.  This is a common concern with new regulations in rapidly evolving fields.\n",
      "\n",
      "**Opportunities:**\n",
      "\n",
      "*   **Enhanced Trust & Transparency:** Compliance with the AI Act can build trust and transparency in AI systems, leading to greater adoption and acceptance by consumers and businesses.  This can create a competitive advantage for companies that prioritize ethical and responsible AI development.\n",
      "*   **Improved AI Quality & Reliability:** The focus on risk management and data governance can lead to improvements in the quality, reliability, and robustness of AI systems.  This can reduce the risk of errors, biases, and unintended consequences.\n",
      "*   **Market Access Advantage:** Companies that demonstrate compliance with the EU AI Act may gain a competitive advantage in the EU market, as customers and partners may prefer to work with organizations that adhere to high ethical and regulatory standards.\n",
      "*   **Innovation in Compliance Tools:** The need to comply with the AI Act will drive innovation in tools and technologies that help organizations manage risk, ensure data governance, and achieve transparency in their AI systems. This will create opportunities for businesses that develop and offer these solutions.\n",
      "*   **Global Influence:** The EU AI Act could set a global standard for AI regulation, influencing other countries to adopt similar frameworks. This could create opportunities for companies that are prepared to comply with these standards in multiple markets.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "*   **Defining \"High-Risk\":**  Clearly defining what constitutes \"high-risk AI\" will be crucial.  Ambiguity in this definition could create uncertainty for businesses and lead to inconsistent enforcement.\n",
      "*   **Enforcement & Oversight:**  Effective enforcement and oversight of the AI Act will be essential to ensure compliance and prevent abuses.  This will require significant resources and expertise on the part of regulators.\n",
      "*   **Balancing Innovation & Regulation:**  Striking the right balance between fostering innovation and regulating AI to protect fundamental rights will be a key challenge.  Overly strict regulations could stifle innovation, while lax enforcement could lead to harms.\n",
      "*   **Real-time Biometric Identification Exceptions:** Clearly defined conditions for any allowance of real-time biometric identification. Scope creep and misinterpretation could allow for abuses.\n",
      "*   **Adaptability:** The rapid pace of AI development poses a challenge to regulators. The AI Act needs to be adaptable and flexible to address new and emerging AI technologies and applications.\n",
      "*   **Global Coordination:** Achieving global coordination on AI regulation will be a challenge, as different countries may have different priorities and values.  This could lead to fragmentation and inconsistencies in the regulatory landscape.\n",
      "\n",
      "In summary, the EU AI Act presents both significant risks and opportunities. The key will be for organizations to proactively address the compliance challenges, embrace the principles of responsible AI development, and leverage the opportunities to build trust and create a competitive advantage.\n",
      "\n",
      "=== Final Answer ===\n",
      " Based on my knowledge, here's a breakdown of the EU AI Act's transparency obligations and a preliminary assessment of GPT's potential compliance:\n",
      "\n",
      "**EU AI Act Transparency Obligations:**\n",
      "\n",
      "The EU AI Act emphasizes transparency, particularly for high-risk AI systems. The specific obligations vary depending on the risk level, but key transparency requirements include:\n",
      "\n",
      "*   **Information to Users:** Providers of AI systems intended to interact with natural persons must inform users that they are interacting with an AI system. This aims to prevent deception and allow users to make informed decisions.\n",
      "*   **Transparency Documentation:** High-risk AI systems must have detailed documentation regarding their capabilities, limitations, and intended purpose. This documentation must be available to regulators and, in some cases, to users.\n",
      "*   **Transparency of Training Data:** For certain AI systems, especially those that generate or manipulate content (like generative AI), there are requirements to disclose that the content was AI-generated and to provide information about the data used to train the AI model. This addresses concerns about copyright infringement and the spread of misinformation.\n",
      "*   **Explainability:** While not always explicitly mandated as \"explainability,\" the Act pushes for AI systems to be understandable to relevant parties. This means providing insights into how the AI system functions and makes decisions, particularly in high-risk contexts where decisions can significantly impact individuals.\n",
      "\n",
      "**GPT and Potential Compliance:**\n",
      "\n",
      "Given these obligations, here's how GPT (and similar large language models) might fare:\n",
      "\n",
      "*   **Interaction Disclosure:** GPT, when used in applications like chatbots, generally includes disclaimers indicating interaction with an AI. This aligns with the Act's requirement to inform users they are interacting with an AI system.\n",
      "*   **Documentation:** GPT's providers (e.g., OpenAI) likely maintain extensive internal documentation on the model's architecture, training data, and intended uses. However, the extent to which this documentation is accessible to regulators and the public will be a key factor in compliance.\n",
      "*   **Training Data Transparency:** Providing information about the training data is a significant challenge for GPT and similar models. The datasets are vast and often include copyrighted material. The EU AI Act's requirements in this area could necessitate changes in how these models are trained and deployed, potentially requiring data provenance tracking and rights clearance mechanisms.\n",
      "*   **Explainability:** GPT's complexity makes it difficult to fully explain its decision-making processes. While research is ongoing to improve the interpretability of large language models, achieving the level of explainability required by the EU AI Act for high-risk applications remains a challenge.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}\"\n",
    "\n",
    "def call_gemini(prompt):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "    try:\n",
    "        res = requests.post(GEMINI_URL, headers=headers, json=payload)\n",
    "        res.raise_for_status()\n",
    "        return res.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"[Gemini API Error] {e}\")\n",
    "        return \"Error: Gemini API call failed.\"\n",
    "\n",
    "# === Run a 3-step prompt chain: summarise -> analyse -> final answer\n",
    "def prompt_chain(context, user_question):\n",
    "    # Step 1: Summarise context\n",
    "    summary_prompt = f\"\"\"\n",
    "Summarise the following information in clear bullet points:\n",
    "\n",
    "{context}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "    summary = call_gemini(summary_prompt).strip()\n",
    "\n",
    "    # Step 2: Analyse the summary\n",
    "    analysis_prompt = f\"\"\"\n",
    "Based on the summary below, identify any key risks, opportunities, or challenges:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Analysis:\n",
    "\"\"\"\n",
    "    analysis = call_gemini(analysis_prompt).strip()\n",
    "\n",
    "    # Step 3: Generate final answer\n",
    "    final_prompt = f\"\"\"\n",
    "You are an expert assistant in AI policy.\n",
    "\n",
    "Using the analysis and context summarised earlier, answer the user's question:\n",
    "\"{user_question}\"\n",
    "\n",
    "If there is not enough information, reply with:\n",
    "\"I cannot answer this confidently based on the provided sources.\"\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    answer = call_gemini(final_prompt).strip()\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"analysis\": analysis,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "# === Example usage ===\n",
    "# Replace with your actual context and question\n",
    "context_text = \"The EU AI Act includes obligations for high-risk AI systems including risk management, data governance, and transparency measures. It also prohibits certain uses of AI such as social scoring or real-time biometric identification.\"\n",
    "user_question = \"What are the obligations the EU AI Act on transparency and does GPT have some compliance?\"\n",
    "\n",
    "result = prompt_chain(context_text, user_question)\n",
    "\n",
    "print(\"=== Summary ===\\n\", result[\"summary\"])\n",
    "print(\"\\n=== Analysis ===\\n\", result[\"analysis\"])\n",
    "print(\"\\n=== Final Answer ===\\n\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd49de-542b-43e9-acc6-26dececdff2e",
   "metadata": {},
   "source": [
    "## 5. Additional: FAISS (Scalable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "762eb0cd-d2aa-46ef-b62c-efa18b7639e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS store for: faiss_eu_ai_act\n",
      "Saved FAISS store to: store_faiss/faiss_eu_ai_act_faiss\n",
      "\n",
      "Building FAISS store for: faiss_hn_story\n",
      "Saved FAISS store to: store_faiss/faiss_hn_story_faiss\n",
      "\n",
      "\n",
      "Combined top 5 results for query:\n",
      "What is happening in AI and what are the legal obligations?\n",
      "\n",
      "Result 1 | Source: faiss_eu_ai_act | Score: 0.49480000138282776\n",
      "se of \n",
      "providing it, upon request, to the AI Office and the national competent authorities;\n",
      "(b) draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to \n",
      "integrate the general-purpose AI model into their AI systems. Without prejudice to the need to observe and protect \n",
      "intellectual property rights and confidential business information or trade secrets in accordance with Union and \n",
      "national law, the information and documentation shall:\n",
      "(i)\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2 | Source: faiss_eu_ai_act | Score: 0.5669000148773193\n",
      "eassessment at the earliest six months after that decision.\n",
      "6.\n",
      "The Commission shall ensure that a list of general-purpose AI models with systemic risk is published and shall keep \n",
      "that list up to date, without prejudice to the need to observe and protect intellectual property rights and confidential \n",
      "business information or trade secrets in accordance with Union and national law.\n",
      "SECTION 2\n",
      "Obligations for providers of general-purpose AI models\n",
      "Article 53\n",
      "Obligations for providers of general-purp\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3 | Source: faiss_eu_ai_act | Score: 0.569599986076355\n",
      "IN AI SYSTEMS\n",
      "Article 50\n",
      "Transparency obligations for providers and deployers of certain AI systems\n",
      "1.\n",
      "Providers shall ensure that AI systems intended to interact directly with natural persons are designed and developed in \n",
      "such a way that the natural persons concerned are informed that they are interacting with an AI system, unless this is \n",
      "obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect, taking \n",
      "into account the circumstances and th\n",
      "--------------------------------------------------------------------------------\n",
      "Result 4 | Source: faiss_hn_story | Score: 0.751800000667572\n",
      "What, exactly, is an 'AI Agent'? Here's a litmus test\n",
      "--------------------------------------------------------------------------------\n",
      "Result 5 | Source: faiss_hn_story | Score: 0.8646000027656555\n",
      "The Tragic Case of Intel AI\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Config\n",
    "datasets = {\n",
    "    \"faiss_eu_ai_act\": \"chunks/eu_ai_act_chunks.parquet\",\n",
    "    \"faiss_hn_story\": \"chunks/hn_story_chunks.parquet\",\n",
    "}\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "output_base = \"store_faiss\"\n",
    "\n",
    "# Step 1: Build and save FAISS vector stores\n",
    "for store_name, parquet_path in datasets.items():\n",
    "    print(f\"Building FAISS store for: {store_name}\")\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    if 'text' not in df.columns:\n",
    "        print(f\"Skipped {store_name} — no 'text' column found.\")\n",
    "        continue\n",
    "\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=row[\"text\"],\n",
    "            metadata={\n",
    "                \"chunk_id\": row.get(\"chunk_id\", i),\n",
    "                \"source\": store_name\n",
    "            }\n",
    "        )\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    vectorstore = FAISS.from_documents(documents, embedding=embedding_function)\n",
    "    save_path = f\"{output_base}/{store_name}_faiss\"\n",
    "    vectorstore.save_local(save_path)\n",
    "    print(f\"Saved FAISS store to: {save_path}\\n\")\n",
    "\n",
    "# Step 2: Load all FAISS stores\n",
    "loaded_stores = {}\n",
    "for store_name in datasets:\n",
    "    query_path = f\"{output_base}/{store_name}_faiss\"\n",
    "    vectorstore = FAISS.load_local(\n",
    "        query_path,\n",
    "        embeddings=embedding_function,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    loaded_stores[store_name] = vectorstore\n",
    "\n",
    "# Step 3: Perform combined query across all stores\n",
    "sample_query = \"What is happening in AI and what are the legal obligations?\"\n",
    "top_k_per_store = 3\n",
    "\n",
    "all_results = []\n",
    "for store_name, store in loaded_stores.items():\n",
    "    results = store.similarity_search_with_score(sample_query, k=top_k_per_store)\n",
    "    for doc, score in results:\n",
    "        doc.metadata[\"store\"] = store_name\n",
    "        all_results.append((doc, score))\n",
    "\n",
    "# Step 4: Sort combined results by similarity score (lower is better)\n",
    "all_results.sort(key=lambda x: x[1])  # score is distance\n",
    "\n",
    "# Step 5: Display top N combined results\n",
    "top_k_combined = 5\n",
    "print(f\"\\nCombined top {top_k_combined} results for query:\\n{sample_query}\\n\")\n",
    "\n",
    "for i, (doc, score) in enumerate(all_results[:top_k_combined], 1):\n",
    "    print(f\"Result {i} | Source: {doc.metadata.get('store')} | Score: {round(score, 4)}\")\n",
    "    print(doc.page_content.strip()[:500])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50cb092-3486-4a6d-8e71-016583a76722",
   "metadata": {},
   "source": [
    "## 6. Flask API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46433ca-95e3-41c3-97da-c1a5973d2ff5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Note:</strong> This API is designed to run as a separate script, located in docker/msin0166-individual-docker.py. If the AWS EC2 free tier instance is still active at the time of marking, the functionality should also be accessible via the endpoint http://ec2-13-40-156-23.eu-west-2.compute.amazonaws.com/rag, as demonstrated in the report with supporting screenshots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10bccd2a-5ac1-4fd5-91ce-2222e7b87d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://10.52.141.81:8080\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import requests\n",
    "import neptune\n",
    "from flask import Flask, request, jsonify\n",
    "from dotenv import load_dotenv\n",
    "from statistics import mean\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "NEPTUNE_API_TOKEN = os.getenv(\"NEPTUNE_API_TOKEN\")\n",
    "NEPTUNE_PROJECT = os.getenv(\"NEPTUNE_PROJECT\")\n",
    "\n",
    "GEMINI_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}\"\n",
    "\n",
    "# Load vector stores once at startup\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_paths = {\n",
    "    \"eu_ai_act\": \"store/eu_ai_act_db\",\n",
    "    \"hn_story\": \"store/hn_story_db\",\n",
    "    \"hn_comment\": \"store/hn_comment_db\",\n",
    "    \"llm_benchs_transparency\": \"store/llm_benchs_transparency_db\"\n",
    "}\n",
    "stores = {\n",
    "    name: Chroma(persist_directory=path, embedding_function=embedding)\n",
    "    for name, path in vector_paths.items()\n",
    "}\n",
    "\n",
    "# Init Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# === Flask endpoint that performs RAG query using Gemini + vector search\n",
    "@app.route(\"/rag\", methods=[\"POST\"])\n",
    "def rag_query():\n",
    "    # Start a new Neptune run for this request\n",
    "    neptune_run = neptune.init_run(\n",
    "        project=NEPTUNE_PROJECT,\n",
    "        api_token=NEPTUNE_API_TOKEN\n",
    "    )\n",
    "    neptune_run[\"debug/start\"] = \"RAG request received\"\n",
    "\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        query = data.get(\"question\")\n",
    "        top_k = int(data.get(\"top_k\", 3))\n",
    "\n",
    "        if not query:\n",
    "            return jsonify({\"error\": \"Missing 'question' field\"}), 400\n",
    "\n",
    "        docs = []\n",
    "        scores = []\n",
    "        \n",
    "        # === Use similarity_search_with_score to retrieve top-k documents from each source\n",
    "        for name, store in stores.items():\n",
    "            results = store.similarity_search_with_score(query, k=top_k)\n",
    "            for doc, score in results:\n",
    "                doc.metadata[\"source\"] = name\n",
    "                doc.metadata[\"similarity\"] = score\n",
    "                docs.append(doc)\n",
    "                scores.append(score)\n",
    "\n",
    "        context = \"\\n\\n\".join([f\"[{doc.metadata['source']}] {doc.page_content.strip()}\" for doc in docs])\n",
    "        avg_similarity = round(mean(scores), 4) if scores else None\n",
    "        min_similarity = round(min(scores), 4) if scores else None\n",
    "\n",
    "        # === Construct Gemini prompt enforcing strict source citation\n",
    "        prompt = f\"\"\"\n",
    "You are an expert assistant in AI policy.\n",
    "\n",
    "Only use the context provided to answer the user's question. Do not invent or guess. If the context is not sufficient, respond with:\n",
    "\"I cannot answer this confidently based on the provided sources.\"\n",
    "\n",
    "Always cite sources using [source].\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "        prompt_version = \"v1.3-strict-citation\"\n",
    "\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        response = requests.post(GEMINI_URL, headers=headers, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            json_resp = response.json()\n",
    "            neptune_run[\"debug/gemini_raw_response\"] = str(json_resp)\n",
    "\n",
    "            answer = json_resp[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "            hallucination_flag = \"I cannot answer\" in answer\n",
    "\n",
    "            # Log to Neptune\n",
    "            neptune_run[\"question\"] = query\n",
    "            neptune_run[\"answer\"] = answer\n",
    "            neptune_run[\"prompt/version\"] = prompt_version\n",
    "            neptune_run[\"prompt/full\"] = prompt\n",
    "            neptune_run[\"retrieval/avg_similarity\"].log(avg_similarity)\n",
    "            neptune_run[\"retrieval/min_similarity\"].log(min_similarity)\n",
    "            neptune_run[\"hallucination/flagged\"] = hallucination_flag\n",
    "            neptune_run[\"sources\"] = list(set(doc.metadata[\"source\"] for doc in docs))\n",
    "            neptune_run[\"context/combined\"] = context\n",
    "\n",
    "            for i, doc in enumerate(docs):\n",
    "                neptune_run[f\"context/chunk_{i}\"] = {\n",
    "                    \"source\": doc.metadata[\"source\"],\n",
    "                    \"similarity\": round(doc.metadata[\"similarity\"], 4),\n",
    "                    \"text\": doc.page_content.strip()\n",
    "                }\n",
    "\n",
    "            return jsonify({\n",
    "                \"question\": query,\n",
    "                \"answer\": answer,\n",
    "                \"sources\": list(set(doc.metadata[\"source\"] for doc in docs)),\n",
    "                \"avg_similarity\": avg_similarity,\n",
    "                \"hallucination_flag\": hallucination_flag\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            neptune_run[\"error/gemini_status\"] = response.status_code\n",
    "            neptune_run[\"error/gemini_text\"] = response.text\n",
    "            return jsonify({\"error\": response.text}), response.status_code\n",
    "\n",
    "    except Exception as e:\n",
    "        neptune_run[\"error/exception\"] = str(e)\n",
    "        return jsonify({\"error\": \"Internal error\", \"details\": str(e)}), 500\n",
    "\n",
    "    finally:\n",
    "        neptune_run.stop()\n",
    "\n",
    "# Run Flask on your preferred port (e.g. 8080)\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
