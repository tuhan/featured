{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db28b9c-e3ee-4d59-81fa-9f1be964c1f0",
   "metadata": {},
   "source": [
    "<h1>MSIN0166: Data Engineering - Group Cousework</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cab2ec-8b44-484b-a391-31003868eae0",
   "metadata": {},
   "source": [
    "Title: State of the Crypto: Cryptocurrency Trends, Economic Correlations, and Sentiment<br/>\n",
    "Authors: Tuhan Sapumanage, Sherlock Pi, Filippo Chen, Alifiya Kanpurwala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4b141-e574-45be-85ae-ab364f748da8",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19676c-1c26-4fea-8780-7cce7ccde490",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Note:</strong> Environment variables are stored securely in the GitHub repository under Codespaces secrets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f2958-44f9-4f96-b838-0fc148daaa97",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de79bd09-81d3-44a0-980b-f89239815332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /opt/conda/lib/python3.12/site-packages (0.2.54)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.12/site-packages (from yfinance) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/conda/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/conda/lib/python3.12/site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/conda/lib/python3.12/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /opt/conda/lib/python3.12/site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/lib/python3.12/site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8bfda4-5751-4c47-a9e4-f1c345807a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33620bc2-2b5c-439a-8f58-612c64bf6250",
   "metadata": {},
   "source": [
    "### Getting Env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b1a1ed-006e-4ca9-a3d3-33b149640888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3dc7fd-cf13-453c-ac04-3ed15784e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ce461-692a-4c24-9a11-6d143bc98639",
   "metadata": {},
   "source": [
    "### Setting up PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f07b423-a47d-4d44-90f0-589f2f4cc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create PostgreSQL connection string\n",
    "pg_conn_string = f\"postgresql+psycopg2://{os.getenv('POSTGRES_USERNAME')}:{os.getenv('POSTGRES_PASSWORD')}@{\"uclba-de25v2.cluster-cowglvndjvxv.eu-west-2.rds.amazonaws.com\"}:{5432}/{\"postgres\"}\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "pg_engine = create_engine(pg_conn_string, connect_args={\"options\": f\"-c search_path={os.getenv('POSTGRES_SCHEMA')}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a95564-5f59-481a-bdea-d722b6b5d7b6",
   "metadata": {},
   "source": [
    "### Setting up MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d321eaca-1034-4454-9cfa-46a1f99109b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "uri = \"mongodb+srv://ucl-de:\"+os.getenv('MONGODB_PASSWORD')+\"@ucl-de.auvdj.mongodb.net/?retryWrites=true&w=majority&appName=ucl-de\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "db = client['ucl-de']\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b50cd-43de-4712-a1f4-69ae66707a45",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e72f49-3613-477c-b013-3a1de55f2625",
   "metadata": {},
   "source": [
    "### Crypto Data - NoSQL/JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e655817-44d9-4fa6-94e0-34c043768c2b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Note:</strong> Only five (5) requests a minute so two loops for 10. Wait 60 seconds before executing 'coin_names_2'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d933216-6499-4b71-9961-900c68fd6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coin_data(coin_names):\n",
    "    for coin_name in coin_names:\n",
    "        # Base URL for Polygon.io API\n",
    "        url = 'https://api.polygon.io/v2/aggs/ticker/X:'+coin_name+'USD/range/1/day/2024-01-01/2024-12-31'\n",
    "        \n",
    "        # Send a GET request to the API\n",
    "        response = requests.get(url, params={'apiKey': os.getenv('POLYGON_API_KEY')})\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(coin_name)\n",
    "            print(data['results'][0])\n",
    "            # Extract the 'results' JSON and rename fields\n",
    "            data_to_save = [{\n",
    "                'volume': result['v'],\n",
    "                'volume_weighted': result['vw'],\n",
    "                'open': result['o'],\n",
    "                'close': result['c'],\n",
    "                'high': result['h'],\n",
    "                'low': result['l'],\n",
    "                'timestamp': pd.to_datetime(result['t'], unit='ms'),\n",
    "                'num_trades': result['n'],\n",
    "                'coin_name': coin_name  # Add the coin_name field\n",
    "            } for result in data['results']]     \n",
    "\n",
    "            # Connect to MongoDB and insert the documents\n",
    "            collection = db['crypto_data']\n",
    "            collection.insert_many(data_to_save)\n",
    "            print(\"Documents inserted successfully.\")\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d063c56d-fbcd-4353-aed9-77f0c2e04235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOL\n",
      "{'v': 1626237.539067202, 'vw': 105.8227, 'o': 101.7, 'c': 110.11, 'h': 110.44, 'l': 101.47, 't': 1704067200000, 'n': 108681}\n",
      "Documents inserted successfully.\n",
      "DOGE\n",
      "{'v': 151978174.98105866, 'vw': 0.0907, 'o': 0.0894535, 'c': 0.09202, 'h': 0.092192, 'l': 0.0885, 't': 1704067200000, 'n': 22997}\n",
      "Documents inserted successfully.\n",
      "XRP\n",
      "{'v': 37059909.13826048, 'vw': 0.6202, 'o': 0.6154, 'c': 0.6299, 'h': 0.63151, 'l': 0.60817, 't': 1704067200000, 'n': 40192}\n",
      "Documents inserted successfully.\n",
      "BTC\n",
      "{'v': 11571.132272252362, 'vw': 43114.6615, 'o': 42241.1, 'c': 44220.78, 'h': 44240.8, 'l': 42175.65, 't': 1704067200000, 'n': 219298}\n",
      "Documents inserted successfully.\n",
      "ETH\n",
      "{'v': 83717.80414219313, 'vw': 2308.4244, 'o': 2281.03, 'c': 2352.76, 'h': 2353.33, 'l': 2264.86, 't': 1704067200000, 'n': 138648}\n",
      "Documents inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "coin_names_1 = {\"BTC\", \"ETH\", \"XRP\", \"SOL\", \"DOGE\"}\n",
    "get_coin_data(coin_names_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce034039-b047-4a55-b74e-9c4a924764f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINK\n",
      "{'v': 1729847.2418694491, 'vw': 15.2438, 'o': 14.934, 'c': 15.56, 'h': 15.585, 'l': 14.812, 't': 1704067200000, 'n': 35351}\n",
      "Documents inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "coin_names_2 = {\"ADA\", \"TRX\", \"XLM\", \"AVAX\", \"LINK\"}\n",
    "get_coin_data(coin_names_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28accd3b-5413-4367-9018-cb66a0640a52",
   "metadata": {},
   "source": [
    "### Stock Market Data - SQL/DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b91d5f16-1547-4ed0-a810-a4339f162774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  11 of 11 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>184.532089</td>\n",
       "      <td>187.315382</td>\n",
       "      <td>182.792533</td>\n",
       "      <td>186.033072</td>\n",
       "      <td>82488700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>149.929993</td>\n",
       "      <td>152.380005</td>\n",
       "      <td>148.389999</td>\n",
       "      <td>151.539993</td>\n",
       "      <td>47339400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>AVGO</td>\n",
       "      <td>107.095505</td>\n",
       "      <td>108.735411</td>\n",
       "      <td>106.277520</td>\n",
       "      <td>107.760543</td>\n",
       "      <td>28831000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>BRK-B</td>\n",
       "      <td>362.459991</td>\n",
       "      <td>362.570007</td>\n",
       "      <td>355.940002</td>\n",
       "      <td>356.320007</td>\n",
       "      <td>4737000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>138.902084</td>\n",
       "      <td>139.952119</td>\n",
       "      <td>137.090672</td>\n",
       "      <td>138.941904</td>\n",
       "      <td>20071900.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price       Date Symbol       Close        High         Low        Open  \\\n",
       "0     2024-01-02   AAPL  184.532089  187.315382  182.792533  186.033072   \n",
       "1     2024-01-02   AMZN  149.929993  152.380005  148.389999  151.539993   \n",
       "2     2024-01-02   AVGO  107.095505  108.735411  106.277520  107.760543   \n",
       "3     2024-01-02  BRK-B  362.459991  362.570007  355.940002  356.320007   \n",
       "4     2024-01-02   GOOG  138.902084  139.952119  137.090672  138.941904   \n",
       "\n",
       "Price      Volume  \n",
       "0      82488700.0  \n",
       "1      47339400.0  \n",
       "2      28831000.0  \n",
       "3       4737000.0  \n",
       "4      20071900.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Stock Data for S&P ETF & Top 10 Companies for 2024-01-01 to 2024-12-31\n",
    "# Define the ticker symbols\n",
    "tickers = ['SPY', 'AAPL', 'MSFT', 'NVDA', 'AMZN', 'META', 'GOOGL', 'TSLA', 'AVGO', 'GOOG', 'BRK-B']\n",
    "\n",
    "# Download historical data for 2024\n",
    "df_stock = yf.download(tickers, start=\"2024-01-01\", end=\"2024-12-31\")\n",
    "\n",
    "# Reset the index to make the date a column and flatten the MultiIndex columns\n",
    "df_stock.columns = ['_'.join(col).strip() for col in df_stock.columns.to_flat_index()]\n",
    "df_stock.reset_index(inplace=True)\n",
    "\n",
    "# Unpivot the DataFrame while keeping columns for 'Date' and 'Symbol'\n",
    "df_stock = df_stock.melt(id_vars=['Date'], var_name='Symbol_Price', value_name='Value')\n",
    "\n",
    "# Extract 'Symbol' and 'Price' from the 'Symbol_Price' column\n",
    "df_stock[['Price', 'Symbol']] = df_stock['Symbol_Price'].str.split('_', expand=True)\n",
    "\n",
    "# Drop the 'Symbol_Price' column\n",
    "df_stock.drop(columns=['Symbol_Price'], inplace=True)\n",
    "\n",
    "# Pivot the table to create a wide format with columns for High, Low, Open, Close, Volume\n",
    "df_stock = df_stock.pivot_table(index=['Date', 'Symbol'], columns='Price', values='Value', aggfunc='first')\n",
    "\n",
    "# Reset the index to flatten the result\n",
    "df_stock.reset_index(inplace=True)\n",
    "\n",
    "# Save the DataFrame to PostgreSQL as 'stock_data' table\n",
    "df_stock.to_sql('stock_data', pg_engine, if_exists='replace', index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "df_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfee3b4-1e6b-4fd4-8b1d-3e3db7b38dde",
   "metadata": {},
   "source": [
    "### Crypto Posts - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f2fbf-3a8a-4864-8f43-b2f77eac1aa8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Note:</strong> The code block below takes a considerable amount of time to execute (approximately 10â€“15 minutes). To streamline the workflow, we pre-executed the code and saved a snapshot of the results as a JSON file (method 2, below). This snapshot can be loaded and used directly; however, if re-execution is required, the code can simply be uncommented and run.<br/><br/>Both a modular and an automated version of this process are available in the GitHub repository, titled web_scrape.ipynb and web_scrape_100_most_recent.ipynb, respectively (with further context provided in Section 2.3, Scalability and Automation Consideration, of this report).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf4a36-a3f8-4a90-8b74-109aaef5bf49",
   "metadata": {},
   "source": [
    "#### Method 1 - Get New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f1c0c-1bda-44aa-a921-250d30b7cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e71cbe0-110e-4381-bc62-3989c3bf43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4f335ea-09ad-4612-955b-0a6df54bceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11dc7eb4-4eee-4f9f-a20a-70cf69b315a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_timestamp(timestamp_str):\n",
    "#     \"\"\"\n",
    "#     Parse a timestamp string (e.g., \"January 02, 2024, 02:36:13 PM\")\n",
    "#     into a datetime object. Adjust the format string if needed.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return datetime.strptime(timestamp_str, \"%B %d, %Y, %I:%M:%S %p\")\n",
    "#     except Exception:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81be8995-0dab-4492-8e7c-3696813bdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_topic_posts(driver, topic_url, subject_title, subsection):\n",
    "#     \"\"\"\n",
    "#     Given a topic URL, visits the page (with ;all appended) and extracts all posts.\n",
    "#     For each post, extracts the timestamp and post content using the provided Relative XPath.\n",
    "#     Returns a list of dictionaries with keys: 'timestamp', 'post', 'subject', 'subsection'.\n",
    "#     \"\"\"\n",
    "#     full_url = topic_url\n",
    "#     driver.get(full_url)\n",
    "#     time.sleep(3)  # Allow time for the page to load\n",
    "\n",
    "#     topic_posts = []\n",
    "#     # Locate all post containers within #quickModForm\n",
    "#     posts = driver.find_elements(\n",
    "#         \"xpath\", \"//*[@id='quickModForm']/table[contains(@class,'bordercolor')]/tbody/tr\"\n",
    "#     )\n",
    "\n",
    "#     if not posts:\n",
    "#         print(\"No posts found on topic page:\", full_url)\n",
    "\n",
    "#     for p in posts:\n",
    "#         try:\n",
    "#             # Extract timestamp using the provided Relative XPath\n",
    "#             timestamp_text = p.find_element(\n",
    "#                 \"xpath\", \".//td[2]/table/tbody/tr/td[2]/div[2]\"\n",
    "#             ).text.strip()\n",
    "#             timestamp_obj = datetime.strptime(timestamp_text, \"%B %d, %Y, %I:%M:%S %p\")\n",
    "#             timestamp_text = timestamp_obj.strftime(\"%Y-%m-%d\")\n",
    "#         except NoSuchElementException:\n",
    "#             timestamp_text = \"No timestamp\"\n",
    "\n",
    "#         try:\n",
    "#             # Extract post content using a more precise XPath\n",
    "#             post_text = p.find_element(\n",
    "#                 \"xpath\", \".//td[2]/div[@class='post']\"\n",
    "#             ).text.strip()\n",
    "#         except NoSuchElementException:\n",
    "#             post_text = \"No content\"\n",
    "\n",
    "#         topic_posts.append({\n",
    "#             \"timestamp\": timestamp_text,\n",
    "#             \"post\": post_text,\n",
    "#             \"subject\": subject_title,\n",
    "#             \"subsection\": subsection\n",
    "#         })\n",
    "\n",
    "#     return topic_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "526da528-37ef-4bef-a1dc-a3e9d1ef80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_subsection_topics(num_pages, board_prefix, start_subject_num, start_date_str, \n",
    "#                              end_date_str, keywords):\n",
    "#     \"\"\"\n",
    "#     Scrapes topics from a specific board (subsection) page that meet criteria:\n",
    "#       - The topic's last post timestamp is within the specified date range.\n",
    "#       - The topic title contains the given keyword (case-insensitive).\n",
    "    \n",
    "#     For each matching topic, the function visits the topic's URL (with ;all appended)\n",
    "#     and extracts all posts (timestamp and post content).\n",
    "#     \"\"\"\n",
    "#     start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "#     end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "    \n",
    "#     # Configure headless Chrome browser\n",
    "#     chrome_options = Options()\n",
    "#     chrome_options.add_argument(\"--headless\")\n",
    "#     driver = webdriver.Chrome(\n",
    "#         service=Service(ChromeDriverManager().install()),\n",
    "#         options=chrome_options\n",
    "#     )\n",
    "    \n",
    "#     all_posts = []\n",
    "    \n",
    "#     # Loop over board pages. Each page URL is formed using the subject_start_num (which increments by 40).\n",
    "#     for page in range(num_pages):\n",
    "#         subject_start_num = start_subject_num + page * 40\n",
    "#         board_url = f\"{board_prefix}{subject_start_num}\"\n",
    "#         print(f\"Accessing board page: {board_url}\")\n",
    "#         driver.get(board_url)\n",
    "#         time.sleep(3)\n",
    "        \n",
    "#         # Extract topic rows from the board page.\n",
    "#         # The CSS selectors here assume that each topic is in a table row within a table with class \"bordercolor\".\n",
    "#         topic_rows = driver.find_elements(By.CSS_SELECTOR, \"#bodyarea table.bordercolor tr\")\n",
    "        \n",
    "#         for row in topic_rows:\n",
    "#             try:\n",
    "#                 # Locate the topic link element\n",
    "#                 link = row.find_element(By.CSS_SELECTOR, \"td a[href*='topic=']\")\n",
    "#                 title = link.text.strip()\n",
    "#                 topic_url = link.get_attribute(\"href\")\n",
    "                \n",
    "#                 # Locate the element that holds the last post timestamp.\n",
    "#                 last_post_elem = row.find_element(By.CSS_SELECTOR, \"td.windowbg2.lastpostcol span\")\n",
    "#                 # Extract the timestamp string (taking only the first line)\n",
    "#                 last_post_text = last_post_elem.text.strip().split(\"\\n\")[0]\n",
    "#                 topic_last_post = parse_timestamp(last_post_text)\n",
    "#                 if topic_last_post is None:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Filter topics by date range\n",
    "#                 if not (start_date <= topic_last_post <= end_date):\n",
    "#                     continue\n",
    "                \n",
    "#                 # Filter by keyword (case-insensitive)\n",
    "#                 #if keyword.lower() not in title.lower():\n",
    "#                 #    subsection = keyword\n",
    "#                 #    continue\n",
    "#                 matched_coin = None\n",
    "#                 for abbr, full_name in keywords.items():\n",
    "#                     if abbr.lower() in title.lower() or full_name.lower() in title.lower():\n",
    "#                         matched_coin = abbr  # Assign the abbreviation (e.g., \"BTC\")\n",
    "#                         break  # Stop checking after the first match\n",
    "                \n",
    "#                 # If no keyword match, skip this topic\n",
    "#                 if not matched_coin:\n",
    "#                     continue\n",
    "                \n",
    "#                 subsection = matched_coin\n",
    "                \n",
    "#                 # Here, we set the subsection name. You might extract this differently.\n",
    "#                 print(f\"Scraping topic: {title} | Last post: {last_post_text}\")\n",
    "                \n",
    "#                 # Scrape all posts from the topic\n",
    "#                 topic_posts = scrape_topic_posts(driver, topic_url, title, subsection)\n",
    "#                 all_posts.extend(topic_posts)\n",
    "#             except Exception as e:\n",
    "#                 # If any error occurs in processing a row, skip it.\n",
    "#                 continue\n",
    "    \n",
    "#     driver.quit()\n",
    "#     df = pd.DataFrame(all_posts)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12a7e3-da02-4a74-bea9-fe645b9e62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitcoin = {\n",
    "#     \"BTC\": \"Bitcoin\"\n",
    "# }\n",
    "\n",
    "# df_bitcoin_posts = scrape_subsection_topics(num_pages=50,\n",
    "#                                            board_prefix=\"https://bitcointalk.org/index.php?board=1.\",\n",
    "#                                            start_subject_num=380, \n",
    "#                                            start_date_str=\"2024-01-01\", \n",
    "#                                            end_date_str=\"2024-12-31\", \n",
    "#                                            keywords=bitcoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba745b-6af2-411e-bfe2-04926db02c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bitcoin_posts.info()\n",
    "# df_bitcoin_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e2c24-0289-44b9-89ad-d0a58ff3dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# altcoin_namelist = {\n",
    "#     \"ETH\": \"Ethereum\",\n",
    "#     \"XRP\": \"Ripple\",\n",
    "#     \"SOL\": \"Solana\",\n",
    "#     \"DOGE\": \"Dogecoin\",\n",
    "#     \"ADA\": \"Cardano\",\n",
    "#     \"TRX\": \"TRON\",\n",
    "#     \"XLM\": \"Stellar\",\n",
    "#     \"AVAX\": \"Avalanche\",\n",
    "#     \"SHIB\": \"Shiba Inu\"\n",
    "# }\n",
    "\n",
    "# df_altcoin_posts = scrape_subsection_topics(num_pages=50, \n",
    "#                                            board_prefix=\"https://bitcointalk.org/index.php?board=67.\",\n",
    "#                                            start_subject_num=400, \n",
    "#                                            start_date_str=\"2024-01-01\", \n",
    "#                                            end_date_str=\"2024-12-31\", \n",
    "#                                            keywords=altcoin_namelist)  # Pass as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b6984-900f-4411-bb84-e2694f32bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_altcoin_posts.info()\n",
    "# df_altcoin_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61dd6c-744a-4d1e-984e-0aa5f0645cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine dataframes\n",
    "# df_crypto_posts = pd.concat([df_bitcoin_posts, df_altcoin_posts], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e8d1b-a1f0-4fb2-9178-e78467a8d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textblob\n",
    "# pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c377860-f51e-421c-afab-ea1beca7a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# # Function to clean text\n",
    "# def clean_text(text):\n",
    "#     # Remove quoted messages (common in forum posts)\n",
    "#     text = re.sub(r'Quote from: .*?\\n', '', text, flags=re.DOTALL)\n",
    "    \n",
    "#     # Remove extra newlines and multiple spaces\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# # Function to compute sentiment polarity and subjectivity\n",
    "# def compute_sentiment(text):\n",
    "#     blob = TextBlob(text)\n",
    "#     return blob.sentiment.polarity, blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63ae2e-22eb-4213-a676-a47cfa3eae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load DataFrame\n",
    "# df_crypto_posts['cleaned_post'] = df_crypto_posts['post'].apply(clean_text)\n",
    "\n",
    "# # Apply sentiment analysis on cleaned text\n",
    "# df_crypto_posts[['polarity', 'subjectivity']] = df_crypto_posts['cleaned_post'].apply(\n",
    "#     lambda txt: pd.Series(compute_sentiment(txt))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d5ff3-03b0-4db5-a370-a411d2482b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the updated DataFrame\n",
    "# df_crypto_posts.info()\n",
    "# print(df_crypto_posts.head())\n",
    "\n",
    "# # Save to json\n",
    "# df_crypto_posts.to_json(\"crypto_posts.json\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a19a7-1ccc-4fe5-b196-db81e4220c75",
   "metadata": {},
   "source": [
    "#### Method 2 - Use Snapshot in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6d1053c-93cb-4983-ae70-185b512524ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('crypto_posts.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize an empty list to store the formatted data\n",
    "crypto_posts = []\n",
    "\n",
    "# Get the keys of the timestamp and post data dynamically\n",
    "timestamp_keys = data.get('timestamp', {}).keys()\n",
    "post_keys = data.get('post', {}).keys()\n",
    "\n",
    "# We will assume both 'timestamp' and 'post' have the same keys\n",
    "# Loop through all available keys\n",
    "for key in timestamp_keys:\n",
    "    # Dynamically create a dictionary combining values from timestamp and post for each key\n",
    "    crypto_posts.append({\n",
    "        **{field: data[field].get(key, '') for field in data if isinstance(data[field], dict)},\n",
    "    })\n",
    "\n",
    "# Print the structured output\n",
    "# print(json.dumps(crypto_posts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975ed48-4d5b-4e88-8c48-930b3ec9dad6",
   "metadata": {},
   "source": [
    "#### Loading results to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5cdffaf-f0b4-4fcc-b0c5-59023fa75763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('67d2be0acf26c16f9920ded5'), 'timestamp': '2024-11-05', 'post': \"Bitcoin has indeed exceeded my expectations. Not only its price has reached too far, but also its popularity globally amazes everyone. No one would thought in the first place that bitcoin will be this hyped and trending, but we all know the excellent potentials of bitcoin, so it's not surprising at all. There's still a lot of positive updates await for bitcoin, and we as bitcoiners, should always look forward for a brighter future waiting ahead of us.\", 'subject': 'Has Bitcoin met your expectations from the time you bought your Bitcoin till now', 'subsection': 'BTC', 'cleaned_post': \"Bitcoin has indeed exceeded my expectations. Not only its price has reached too far, but also its popularity globally amazes everyone. No one would thought in the first place that bitcoin will be this hyped and trending, but we all know the excellent potentials of bitcoin, so it's not surprising at all. There's still a lot of positive updates await for bitcoin, and we as bitcoiners, should always look forward for a brighter future waiting ahead of us.\", 'polarity': 0.1534090909, 'subjectivity': 0.5629734848}\n",
      "Data successfully saved\n"
     ]
    }
   ],
   "source": [
    "# Set up MongoDB connection\n",
    "# Other parts already set above\n",
    "collection = db['crypto_posts']  # Replace with your collection name\n",
    "\n",
    "# Insert the data into the collection\n",
    "collection.insert_many(crypto_posts)\n",
    "\n",
    "# Check if data is saved by querying the collection\n",
    "saved_data = collection.find().limit(5)  # Get the first 5 documents for verification\n",
    "\n",
    "# Print out the saved data\n",
    "print(\"Data successfully saved. Sample:\")\n",
    "print(saved_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846891ce-1384-49a2-9ee0-c2e6c8c79b7a",
   "metadata": {},
   "source": [
    "## Transformation & Loading - Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285cee9d-976b-4af3-bbb1-b3a144137a36",
   "metadata": {},
   "source": [
    "### Setting up PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543cef04-de32-4797-9282-637b08e49983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "mongo_package_name = \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\"\n",
    "postgress_package_name = \"org.postgresql:postgresql:42.3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f5be28-4f83-4fe8-8d4d-5a68db0cdbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.5.4-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0b7406a4-ae5f-4890-8257-e71490d46712;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.3.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 706ms :: artifacts dl 76ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.postgresql#postgresql;42.3.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0b7406a4-ae5f-4890-8257-e71490d46712\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/25ms)\n",
      "25/03/13 13:36:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars.packages\", f\"{postgress_package_name},{mongo_package_name}\") \\\n",
    "    .appName(\"PostgresMongo\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af33a9-ec20-4674-895a-694a48800617",
   "metadata": {},
   "source": [
    "### Getting from MongoDB - Crypto Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd781c7-e9c4-4c2d-8149-15430aaf36bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+--------+----------+--------+-------------------+--------------------+---------------+\n",
      "|   close|coin_name|    high|     low|num_trades|    open|          timestamp|              volume|volume_weighted|\n",
      "+--------+---------+--------+--------+----------+--------+-------------------+--------------------+---------------+\n",
      "| 0.14146|      TRX|0.141525|0.140002|      2382|0.140538|2024-03-01 00:00:00|1.1768382651784228E7|          0.141|\n",
      "|0.114479|      TRX| 0.11455| 0.11311|      1224| 0.11344|2024-06-04 01:00:00|   2260126.626597458|         0.1139|\n",
      "|   25.69|     AVAX|   25.91|   24.01|     31005|    25.7|2024-08-01 01:00:00|   578591.4245804062|          25.11|\n",
      "|  0.0953|      XLM|0.097527|  0.0947|     17095|0.097196|2024-10-21 01:00:00| 4.985202882246723E7|         0.0963|\n",
      "|  0.5231|      ADA|  0.5435|  0.5073|     41020| 0.54192|2024-01-06 00:00:00| 3.461921054990041E7|          0.523|\n",
      "+--------+---------+--------+--------+----------+--------+-------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_crypto = spark.read.format(\"mongo\").option(\"uri\", \"mongodb+srv://ucl-de:\"+os.getenv('MONGODB_PASSWORD')+\"@ucl-de.auvdj.mongodb.net/ucl-de.crypto_data?retryWrites=true&w=majority\").load()\n",
    "df_crypto = df_crypto.drop(\"_id\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_crypto = df_crypto.dropDuplicates()\n",
    "\n",
    "df_crypto.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae832b-87b9-4b78-bda1-3d2a81ff8a94",
   "metadata": {},
   "source": [
    "### Getting from PostgreSQL - Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc0d76d-0930-4e91-897d-aa2faa559ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------------+------------------+------------------+------------------+---------+\n",
      "|               Date|Symbol|             Close|              High|               Low|              Open|   Volume|\n",
      "+-------------------+------+------------------+------------------+------------------+------------------+---------+\n",
      "|2024-01-29 00:00:00|  AMZN|161.25999450683594| 161.2899932861328|158.89999389648438|159.33999633789062|4.52704E7|\n",
      "|2024-02-05 00:00:00|  GOOG| 144.2467803955078|145.97858335516779| 143.2315996965748|143.36097653848321|2.92544E7|\n",
      "|2024-03-12 00:00:00|  AVGO| 127.4710464477539| 129.0409048326896|124.25437096442687|128.96294489563127| 4.2789E7|\n",
      "|2024-03-19 00:00:00|  AMZN|175.89999389648438|176.08999633789062|173.52000427246094|174.22000122070312|2.68809E7|\n",
      "|2024-03-20 00:00:00|  AVGO|126.44035339355469| 127.6284542487443|121.54425595762277|122.77596322516155| 4.0946E7|\n",
      "+-------------------+------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# PostgreSQL connection details\n",
    "pg_url = f\"jdbc:postgresql://uclba-de25v2.cluster-cowglvndjvxv.eu-west-2.rds.amazonaws.com:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": os.getenv(\"POSTGRES_USERNAME\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"searchpath\": os.getenv(\"POSTGRES_SCHEMA\")  # Optional, specify schema if necessary\n",
    "}\n",
    "\n",
    "# Define the table name\n",
    "table_name = \"stock_data\"  # Replace with your table name\n",
    "\n",
    "# Form the SQL query\n",
    "query = f\"SELECT * FROM {os.getenv(\"POSTGRES_SCHEMA\")}.{table_name}\"\n",
    "\n",
    "# Load data from PostgreSQL into Spark DataFrame\n",
    "df_stocks = spark.read.jdbc(url=pg_url, table=f\"({query}) as query\", properties=pg_properties)\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_stocks = df_stocks.dropDuplicates()\n",
    "\n",
    "# Show the first 5 rows of the dataframe\n",
    "df_stocks.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc74e6-4c38-4dd8-bb17-21a1ab3d1709",
   "metadata": {},
   "source": [
    "### Getting from MongoDB - Crypto Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "551706b1-de79-4c6e-88b7-1e206b9b1f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+------------+----------+----------+\n",
      "|        cleaned_post|    polarity|                post|             subject|subjectivity|subsection| timestamp|\n",
      "+--------------------+------------+--------------------+--------------------+------------+----------+----------+\n",
      "|I'm into crypto s...|0.1349403122|I'm into crypto s...|Is bitcoin slowly...|0.4847337006|       BTC|2024-12-13|\n",
      "|Don't forget, gol...|   0.0090625|Quote from: slama...|Bitcoin now more ...|     0.63625|       BTC|2024-11-15|\n",
      "|The excitement wi...| 0.096969697|The excitement wi...|Hiccups that can ...|0.4901515152|       BTC|2024-10-10|\n",
      "|It depends on you...|0.1594238683|Quote from: uneng...|Suggest Practical...|0.4605349794|       BTC|2024-07-14|\n",
      "|Where do you stan...|      0.1875|Quote from: IceLi...|Regulating Bitcoi...|0.6356818182|       BTC|2024-07-02|\n",
      "+--------------------+------------+--------------------+--------------------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_posts = spark.read.format(\"mongo\").option(\"uri\", \"mongodb+srv://ucl-de:\"+os.getenv('MONGODB_PASSWORD')+\"@ucl-de.auvdj.mongodb.net/ucl-de.crypto_posts?retryWrites=true&w=majority\").load()\n",
    "df_posts = df_posts.drop(\"_id\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_posts = df_posts.dropDuplicates()\n",
    "\n",
    "df_posts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4a6522d-2335-48a5-b7cd-20b1bcea89e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+------------+----------+-------------------+\n",
      "|        cleaned_post|    polarity|                post|             subject|subjectivity|subsection|          timestamp|\n",
      "+--------------------+------------+--------------------+--------------------+------------+----------+-------------------+\n",
      "|I'm into crypto s...|0.1349403122|I'm into crypto s...|Is bitcoin slowly...|0.4847337006|       BTC|2024-12-13 00:00:00|\n",
      "|Don't forget, gol...|   0.0090625|Quote from: slama...|Bitcoin now more ...|     0.63625|       BTC|2024-11-15 00:00:00|\n",
      "|The excitement wi...| 0.096969697|The excitement wi...|Hiccups that can ...|0.4901515152|       BTC|2024-10-10 00:00:00|\n",
      "|For now, we can a...|     0.14875|Quote from: Wakat...|All eyes On Trump...|0.5068055556|       BTC|2024-07-25 00:00:00|\n",
      "|It depends on you...|0.1594238683|Quote from: uneng...|Suggest Practical...|0.4605349794|       BTC|2024-07-14 00:00:00|\n",
      "+--------------------+------------+--------------------+--------------------+------------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cast 'timestamp' column to TIMESTAMP type if it's in string format\n",
    "df_posts = df_posts.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Now show the updated dataframe\n",
    "df_posts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef25322-50de-4786-83b8-8f2eebd9efec",
   "metadata": {},
   "source": [
    "### Setting Up S3 for Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353bfbd-167a-4b05-ac59-876b28a26e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for connecting to AWS S3\n",
    "!pip install --user boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2fc0b4f-8c03-4841-b186-ba42ec4e8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get S3 credentials stored in Secrets Manager\n",
    "def get_s3_credentials(secret_name=\"S3_access\", region=\"eu-north-1\"):\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=os.environ[\"AWS_REGION\"]\n",
    "    )\n",
    "    client = session.client(\"secretsmanager\")\n",
    "    response = client.get_secret_value(SecretId=secret_name)\n",
    "    secret = json.loads(response[\"SecretString\"])\n",
    "    return secret[\"AWS_ACCESS_KEY_ID\"], secret[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "# Run Spark query, save to Parquet, upload to S3\n",
    "def execute_query_and_upload_to_s3(query, parquet_file_name):\n",
    "    from pyspark.sql import SparkSession\n",
    "    import os\n",
    "\n",
    "    # Execute Spark SQL query\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    df = spark.sql(query)\n",
    "\n",
    "    # Save result to local parquet file (single file)\n",
    "    local_dir = f\"/tmp/{parquet_file_name}_dir\"\n",
    "    df = spark.sql(query)\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(local_dir)\n",
    "    print(f\"Query results saved locally at: {local_dir}\")\n",
    "\n",
    "    # Find the Parquet file within directory\n",
    "    for filename in os.listdir(local_dir):\n",
    "        if filename.endswith(\".parquet\"):\n",
    "            parquet_path = os.path.join(local_dir, filename)\n",
    "            final_local_path = f\"/tmp/{parquet_file_name}\"\n",
    "            os.rename(parquet_path, final_local_path)\n",
    "            break\n",
    "\n",
    "    print(f\"Final parquet file path: {final_local_path}\")\n",
    "\n",
    "    # Retrieve S3 credentials\n",
    "    s3_access_key, s3_secret_key = get_s3_credentials()\n",
    "\n",
    "    # Create S3 client\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "\n",
    "    s3_key = f\"data/{parquet_file_name}\"\n",
    "\n",
    "    # Upload to S3\n",
    "    try:\n",
    "        s3.upload_file(final_local_path, BUCKET_NAME, s3_key)\n",
    "        print(f\"File uploaded successfully: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error uploading file to S3:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836f10a-1143-427b-a54d-c0acb1bd54c7",
   "metadata": {},
   "source": [
    "## Transformation - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e65d92-7fa7-40e4-92d6-8ee6f832892f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Notes:</strong>\n",
    "    <ul><li>For efficiency and clarity, we've selected five of the most impactful transformations (2.5, 3.1, 4.1, 8.2 and 8.3), which have been saved as Parquet files and uploaded to S3. The narratives within the attached report correspond directly to these selections. Additionally, provided below is an inventory of over 30 other transformations and extractions, which can similarly be stored and accessed if required.</li>\n",
    "    <li>As data engineers, it is essential to bridge technical findings with accessible language. To support decision-making, illustrative narratives have been included immediately after each technical section, providing clear, concise summaries for decision-makers</li></ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875d9e1-090b-48ec-bc4a-089f227eb638",
   "metadata": {},
   "source": [
    "### 1. Understandng Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70ae081f-f5a6-4b78-90c0-1b42fe8f03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.createOrReplaceTempView(\"stocks\")\n",
    "df_crypto.createOrReplaceTempView(\"crypto\")\n",
    "df_posts.createOrReplaceTempView(\"posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6df7d6d9-30cb-4a29-a2f3-37248ff54785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+\n",
      "|total_rows|         start_date|           end_date|\n",
      "+----------+-------------------+-------------------+\n",
      "|      2761|2024-01-02 00:00:00|2024-12-30 00:00:00|\n",
      "+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) AS total_rows,\n",
    "        MIN(Date) AS start_date,\n",
    "        MAX(Date) AS end_date\n",
    "    FROM stocks\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1045366-499d-4e55-b2e7-e079e1f8b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+\n",
      "|total_rows|         start_date|           end_date|\n",
      "+----------+-------------------+-------------------+\n",
      "|     44286|2024-01-01 00:00:00|2024-12-31 00:00:00|\n",
      "+----------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) AS total_rows,\n",
    "        MIN(timestamp) AS start_date,\n",
    "        MAX(timestamp) AS end_date\n",
    "    FROM crypto\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ef2fdcf-63d6-4e7f-a00e-748eb62aa5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+\n",
      "|total_rows|         start_date|           end_date|\n",
      "+----------+-------------------+-------------------+\n",
      "|      3052|2010-10-19 00:00:00|2024-12-30 00:00:00|\n",
      "+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) AS total_rows,\n",
    "        MIN(CAST(timestamp AS TIMESTAMP)) AS start_date,\n",
    "        MAX(CAST(timestamp AS TIMESTAMP)) AS end_date\n",
    "    FROM posts\n",
    "    WHERE timestamp IS NOT NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5aaea-93ec-4c08-bf32-040370697783",
   "metadata": {},
   "source": [
    "We begin by inspecting the time range and volume of records across our three datasets:\n",
    "\n",
    "- **Crypto**: 3,660 records from **2024-01-01** to **2024-12-31** â€” daily records for the period; higher frequency because crypto exchanges do not close on holidays \n",
    "- **Stocks**: 2,761 records spanning from **2024-01-02** to **2024-12-30** â€” confirms daily entries across multiple companies for a full year (excluding holidays, when the exchanges are closed).\n",
    "- **Posts**: 1,432 sentiment-tagged posts ranging from **2010-10-19** to **2024-12-30** â€” though historical, we'll likely focus on 2024 to align with price data.\n",
    "This confirms all datasets have overlapping coverage for **2024**, making them suitable for correlation and trend analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8cf9b-e59b-4780-81a2-914cde292dfd",
   "metadata": {},
   "source": [
    "### 2. Overviews\n",
    "#### 2.1 Top traded stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbee5610-9a5c-4914-b315-c0353f6ecab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|Symbol|total_traded_volume|\n",
      "+------+-------------------+\n",
      "|  NVDA|       9.4937251E10|\n",
      "|  TSLA|      2.38213787E10|\n",
      "|   SPY|      1.44110064E10|\n",
      "|  AAPL|      1.43565874E10|\n",
      "|  AMZN|      1.03029356E10|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top traded stocks\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Symbol, SUM(Volume) AS total_traded_volume\n",
    "    FROM stocks\n",
    "    GROUP BY Symbol\n",
    "    ORDER BY total_traded_volume DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e94d7b0-b470-400c-b9d3-6e396b2a7dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|coin_name| total_traded_volume|\n",
      "+---------+--------------------+\n",
      "|     DOGE|3.936139643772202E12|\n",
      "|      XRP|7.842841752612482E11|\n",
      "|      ADA|1.159059070260934...|\n",
      "|      XLM| 7.91642754160789E10|\n",
      "|      TRX|2.149682109878055...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Top traded crypto\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name, SUM(volume) AS total_traded_volume\n",
    "    FROM crypto\n",
    "    GROUP BY coin_name\n",
    "    ORDER BY total_traded_volume DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673c1ad-c87f-4730-88f6-d429f685eaee",
   "metadata": {},
   "source": [
    "#### 2.2 Daily Price Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade3b9a7-7aa8-41a5-b8f0-42356d00e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+\n",
      "|Symbol|               Date|         price_gap|\n",
      "+------+-------------------+------------------+\n",
      "|  AAPL|2024-01-02 00:00:00| 4.522848372763065|\n",
      "|  AMZN|2024-01-02 00:00:00|3.9900054931640625|\n",
      "|  AVGO|2024-01-02 00:00:00|2.4578913357388643|\n",
      "| BRK-B|2024-01-02 00:00:00|   6.6300048828125|\n",
      "|  GOOG|2024-01-02 00:00:00| 2.864706685965018|\n",
      "| GOOGL|2024-01-02 00:00:00| 2.959273593909728|\n",
      "|  META|2024-01-02 00:00:00|13.099953782164846|\n",
      "|  MSFT|2024-01-02 00:00:00| 9.044104012460366|\n",
      "|  NVDA|2024-01-02 00:00:00|1.6995150988554357|\n",
      "|   SPY|2024-01-02 00:00:00|3.1397485945930157|\n",
      "|  TSLA|2024-01-02 00:00:00| 6.839996337890625|\n",
      "|  AAPL|2024-01-03 00:00:00| 2.435390462707147|\n",
      "|  AMZN|2024-01-03 00:00:00| 2.720001220703125|\n",
      "|  AVGO|2024-01-03 00:00:00|1.9428331710988118|\n",
      "| BRK-B|2024-01-03 00:00:00| 6.779998779296875|\n",
      "|  GOOG|2024-01-03 00:00:00| 2.650480301344345|\n",
      "| GOOGL|2024-01-03 00:00:00|2.5407923261034284|\n",
      "|  META|2024-01-03 00:00:00|  4.75186803970297|\n",
      "|  MSFT|2024-01-03 00:00:00|4.7053088974220145|\n",
      "|  NVDA|2024-01-03 00:00:00|0.8637535543833152|\n",
      "+------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In stocks\n",
    "spark.sql(\"\"\"\n",
    "   SELECT Symbol, Date, (High - Low) AS price_gap \n",
    "   FROM stocks\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59abb02f-ff61-4537-9ff6-658ee22d6d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+\n",
      "|coin_name|      date|         price_gap|\n",
      "+---------+----------+------------------+\n",
      "|      SOL|2024-01-01| 8.969999999999999|\n",
      "|      SOL|2024-01-02|11.030000000000001|\n",
      "|      SOL|2024-01-03|39.040000000000006|\n",
      "|      SOL|2024-01-04|11.590000000000003|\n",
      "|      SOL|2024-01-05|10.239999999999995|\n",
      "|      SOL|2024-01-06| 8.820000000000007|\n",
      "|      SOL|2024-01-07|  9.22999999999999|\n",
      "|      SOL|2024-01-08|16.049999999999997|\n",
      "|      SOL|2024-01-09|13.260000000000005|\n",
      "|      SOL|2024-01-10|13.530000000000001|\n",
      "|      SOL|2024-01-11|              9.75|\n",
      "|      SOL|2024-01-12|14.227999999999994|\n",
      "|      SOL|2024-01-13| 8.010000000000005|\n",
      "|      SOL|2024-01-14|11.829999999999998|\n",
      "|      SOL|2024-01-15|3.9680000000000035|\n",
      "|      SOL|2024-01-16| 4.658000000000001|\n",
      "|      SOL|2024-01-17|6.6299999999999955|\n",
      "|      SOL|2024-01-18|12.189999999999998|\n",
      "|      SOL|2024-01-19| 8.599999999999994|\n",
      "|      SOL|2024-01-20| 4.879000000000005|\n",
      "+---------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Crypto\n",
    "spark.sql(\"\"\"\n",
    "   SELECT coin_name, DATE(timestamp) AS date, (high - low) AS price_gap\n",
    "   FROM crypto\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654578f-652c-4619-9f10-75d9072d1d43",
   "metadata": {},
   "source": [
    "#### 2.4 Monthly Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0be925c2-5a1e-499a-9be1-f0060c8dc802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+\n",
      "|Symbol|              month|monthly_volatility|\n",
      "+------+-------------------+------------------+\n",
      "|  NVDA|2024-03-01 00:00:00|  3.19225511782661|\n",
      "|  AMZN|2024-08-01 00:00:00| 6.445405383445364|\n",
      "|  AVGO|2024-12-01 00:00:00|  31.6445725773839|\n",
      "|  TSLA|2024-11-01 00:00:00| 34.60325024954025|\n",
      "|   SPY|2024-03-01 00:00:00|5.3122021266211235|\n",
      "|  AAPL|2024-04-01 00:00:00|  2.75979884664453|\n",
      "|  MSFT|2024-10-01 00:00:00| 6.505105737121832|\n",
      "|   SPY|2024-04-01 00:00:00| 8.037746030590498|\n",
      "|  AAPL|2024-08-01 00:00:00| 6.894340138490561|\n",
      "| GOOGL|2024-01-01 00:00:00|5.3201708695985905|\n",
      "|   SPY|2024-08-01 00:00:00|15.359290788146952|\n",
      "|   SPY|2024-02-01 00:00:00| 5.617393502720848|\n",
      "|  AVGO|2024-03-01 00:00:00|5.4289384974893045|\n",
      "|  META|2024-02-01 00:00:00|20.360763034805082|\n",
      "|   SPY|2024-05-01 00:00:00| 8.381732485560372|\n",
      "| BRK-B|2024-09-01 00:00:00| 7.740633561813219|\n",
      "|  AVGO|2024-07-01 00:00:00|  9.31982569561325|\n",
      "|  AAPL|2024-03-01 00:00:00|2.8734643351689013|\n",
      "|  TSLA|2024-01-01 00:00:00|20.442824018402018|\n",
      "|  GOOG|2024-01-01 00:00:00| 5.323172024475695|\n",
      "+------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Stocks\n",
    "spark.sql(\"\"\"\n",
    "   SELECT Symbol, DATE_TRUNC('month', Date) AS month, STDDEV(Close) AS monthly_volatility \n",
    "   FROM stocks \n",
    "   GROUP BY Symbol, month\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b3bb736-5969-41af-b121-0be279f520e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+\n",
      "|coin_name|              month|  monthly_volatility|\n",
      "+---------+-------------------+--------------------+\n",
      "|     DOGE|2024-12-01 00:00:00| 0.05146356701032425|\n",
      "|      DOT|2024-09-01 00:00:00|  0.2517486375449671|\n",
      "|      ETH|2024-11-01 00:00:00|   364.9937401275454|\n",
      "|      BTC|2024-03-01 00:00:00|   3109.738688745365|\n",
      "|      ADA|2024-01-01 00:00:00|0.037844254610782065|\n",
      "|      DOT|2024-07-01 00:00:00|  0.3036426374709631|\n",
      "|      TRX|2024-07-01 00:00:00|0.004066681153474227|\n",
      "|      ADA|2024-12-01 00:00:00|  0.1285970226814527|\n",
      "|      XLM|2024-10-01 00:00:00|0.002259648524964068|\n",
      "|      ADA|2024-09-01 00:00:00| 0.02518933206245201|\n",
      "|      BTC|2024-02-01 00:00:00|   5125.139363336347|\n",
      "|      SOL|2024-06-01 00:00:00|  13.106615748238415|\n",
      "|      TRX|2024-11-01 00:00:00|0.018150246206198057|\n",
      "|      DOT|2024-10-01 00:00:00| 0.13797729570125747|\n",
      "|      ETH|2024-02-01 00:00:00|  333.45818930924503|\n",
      "|      SOL|2024-04-01 00:00:00|   18.93278849188849|\n",
      "|      DOT|2024-04-01 00:00:00|  0.9076205246586306|\n",
      "|      TRX|2024-12-01 00:00:00| 0.04124533352752826|\n",
      "|      ETH|2024-10-01 00:00:00|   106.1508125600587|\n",
      "|     AVAX|2024-05-01 00:00:00|  2.3013531032715457|\n",
      "+---------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#crypto\n",
    "spark.sql(\"\"\"\n",
    "   SELECT coin_name, DATE_TRUNC('month', timestamp) AS month, STDDEV(close) AS monthly_volatility\n",
    "   FROM crypto\n",
    "   GROUP BY coin_name, month\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5916b-8de2-4918-9c42-b0afd709eb5a",
   "metadata": {},
   "source": [
    "#### 2.5 Volume Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9944bb9-8839-44a9-b9ad-88bbca027f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required functions\n",
    "from pyspark.sql.functions import avg, col, round as round_\n",
    "from pyspark.sql.window import Window\n",
    "#stock\n",
    "window_stock = Window.partitionBy(\"Symbol\").orderBy(\"Date\").rowsBetween(-7, -1)\n",
    "\n",
    "df_stocks = df_stocks.withColumn(\"avg_weekly_vol\", avg(\"Volume\").over(window_stock)) \\\n",
    "                     .withColumn(\"volume_spike\", col(\"Volume\") / col(\"avg_weekly_vol\"))\n",
    "\n",
    "df_stocks.createOrReplaceTempView(\"stocks\")\n",
    "\n",
    "#crypto\n",
    "window_crypto = Window.partitionBy(\"coin_name\").orderBy(\"timestamp\").rowsBetween(-7, -1)\n",
    "\n",
    "df_crypto = df_crypto.withColumn(\"avg_weekly_vol\", avg(\"volume\").over(window_crypto)) \\\n",
    "                     .withColumn(\"volume_spike\", col(\"volume\") / col(\"avg_weekly_vol\"))\n",
    "\n",
    "df_crypto.createOrReplaceTempView(\"crypto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f74c2e2-198e-432c-89ca-1aeb2c035e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------------+-----+\n",
      "|asset|               date|volume_spike| type|\n",
      "+-----+-------------------+------------+-----+\n",
      "| AAPL|2024-09-20 00:00:00|        6.37|Stock|\n",
      "| META|2024-02-02 00:00:00|        4.51|Stock|\n",
      "| META|2024-04-25 00:00:00|        4.39|Stock|\n",
      "| META|2024-12-20 00:00:00|        4.15|Stock|\n",
      "| AVGO|2024-12-13 00:00:00|        3.93|Stock|\n",
      "| GOOG|2024-06-21 00:00:00|        3.84|Stock|\n",
      "| TSLA|2024-10-24 00:00:00|        3.72|Stock|\n",
      "| AMZN|2024-08-02 00:00:00|         3.3|Stock|\n",
      "| AVGO|2024-05-31 00:00:00|        3.29|Stock|\n",
      "|BRK-B|2024-12-20 00:00:00|        3.18|Stock|\n",
      "+-----+-------------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#stocks\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Symbol AS asset, Date AS date, ROUND(volume_spike, 2) AS volume_spike, 'Stock' AS type\n",
    "    FROM stocks\n",
    "    WHERE volume_spike IS NOT NULL\n",
    "    ORDER BY volume_spike DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd654d4d-2af6-4b8b-a2c9-7968f248d634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------------+------+\n",
      "|asset|               date|volume_spike|  type|\n",
      "+-----+-------------------+------------+------+\n",
      "|  TRX|2024-12-03 00:00:00|       15.97|Crypto|\n",
      "|  TRX|2024-08-20 01:00:00|        8.53|Crypto|\n",
      "|  ETH|2024-08-05 01:00:00|        7.38|Crypto|\n",
      "| LINK|2024-08-05 01:00:00|        6.71|Crypto|\n",
      "| AVAX|2024-06-22 01:00:00|        6.28|Crypto|\n",
      "| DOGE|2024-02-28 00:00:00|         6.0|Crypto|\n",
      "| AVAX|2024-04-13 01:00:00|        5.94|Crypto|\n",
      "|  ADA|2024-11-10 00:00:00|        5.93|Crypto|\n",
      "|  XRP|2024-11-12 00:00:00|         5.6|Crypto|\n",
      "| LINK|2024-04-12 01:00:00|        5.47|Crypto|\n",
      "+-----+-------------------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#crypto\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name AS asset, timestamp AS date, ROUND(volume_spike, 2) AS volume_spike, 'Crypto' AS type\n",
    "    FROM crypto\n",
    "    WHERE volume_spike IS NOT NULL\n",
    "    ORDER BY volume_spike DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6f549-ded4-4de9-aa6b-eff55da23d68",
   "metadata": {},
   "source": [
    "#### Loading - 2.5 Volume Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc6330-ecaa-42f5-8d11-575a844a1c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results saved locally at: /tmp/crypto_top_volume_spikes.parquet_dir\n",
      "Final parquet file path: /tmp/crypto_top_volume_spikes.parquet\n",
      "File uploaded successfully: s3://ucl-de/data/crypto_top_volume_spikes.parquet\n"
     ]
    }
   ],
   "source": [
    "AWS_REGION = os.environ['AWS_REGION'] \n",
    "BUCKET_NAME = os.environ['BUCKET_NAME'] \n",
    "\n",
    "query_to_s3 = \"\"\"\n",
    "SELECT coin_name AS asset, timestamp AS date, ROUND(volume_spike, 2) AS volume_spike, 'Crypto' AS type\n",
    "FROM crypto\n",
    "WHERE volume_spike IS NOT NULL\n",
    "ORDER BY volume_spike DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Define the filename for storing crypto volume spike results as a Parquet file\n",
    "parquet_file_name = \"crypto_top_volume_spikes.parquet\"\n",
    "\n",
    "# Execute the provided SQL query, store results locally as Parquet,\n",
    "# and then upload the file to AWS S3\n",
    "execute_query_and_upload_to_s3(query_to_s3, parquet_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc4ad1-6cdd-4987-bc14-0e048c50c689",
   "metadata": {},
   "source": [
    "**Overviews - Summary**\n",
    "\n",
    "Our EDA dives into key activity and volatility metrics across both stock and crypto markets for 2024, offering a cross-asset perspective.\n",
    "\n",
    "**Trading Volume**\n",
    "- Among stocks, **NVDA**, **TSLA**, and **SPY** dominate in terms of total traded volume â€” aligning with their popularity and market visibility.\n",
    "- In crypto, **DOGE** surprisingly leads, followed by **XRP** and **ADA** â€” highlighting how community-driven or meme-based assets can overshadow even large-cap coins in trading activity.\n",
    "\n",
    "**Daily Price Gap (High - Low)**\n",
    "- For stocks, significant price gaps were observed in **META**, **TSLA**, and **MSFT**, indicating reactive price movement â€” possibly driven by news, earnings, or macro events.\n",
    "- For crypto, **SOLANA (SOL)** showed large price swings across multiple consecutive days, showing intense volatility and potential for high intraday gains or losses.\n",
    "\n",
    "**Monthly Volatility**\n",
    "- Stocks like **TSLA (Nov)** and **AVGO (Dec)** posted extreme standard deviations in closing prices, marking them as high-risk, high-opportunity assets during those windows.\n",
    "- Crypto volatility peaked with **ETH**, **BTC**, and **SOL**, particularly in November and December â€” potentially tied to market speculation or ecosystem events.\n",
    "\n",
    "**Volume Spikes (Abnormal Activity)**\n",
    "- For stocks, names like **AAPL**, **META**, and **GOOG** showed days with trading volumes 3xâ€“6x above their 7-day average â€” ideal candidates for identifying news or investor sentiment shifts.\n",
    "- In crypto, **ADA**, **SOL**, and **TRX** showed enormous spikes (some >100x their weekly average) â€” usually a sign of breakout trading, whale moves, or news-triggered hype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2317b9-8ef4-46b3-ab10-ea795fc27220",
   "metadata": {},
   "source": [
    "### 3. Performance Metrics\n",
    "#### 3.1 Daily Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67e2c4f0-30a1-4212-90d7-02980a474dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, col, first, last, weekofyear\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Stocks: Daily Return\n",
    "window_stock = Window.partitionBy(\"Symbol\").orderBy(\"Date\")\n",
    "df_stocks = df_stocks.withColumn(\n",
    "    \"daily_return\",\n",
    "    (col(\"Close\") - lag(\"Close\").over(window_stock)) / lag(\"Close\").over(window_stock)\n",
    ")\n",
    "\n",
    "# Crypto: Daily Return\n",
    "window_crypto = Window.partitionBy(\"coin_name\").orderBy(\"timestamp\")\n",
    "df_crypto = df_crypto.withColumn(\n",
    "    \"daily_return\",\n",
    "    (col(\"close\") - lag(\"close\").over(window_crypto)) / lag(\"close\").over(window_crypto)\n",
    ")\n",
    "\n",
    "df_stocks.createOrReplaceTempView(\"stocks\")\n",
    "df_crypto.createOrReplaceTempView(\"crypto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab26f7de-c7b3-4aae-a587-1f7dfd255034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------+\n",
      "|Symbol|               Date|daily_return|\n",
      "+------+-------------------+------------+\n",
      "|  AAPL|2024-01-03 00:00:00|     -0.0075|\n",
      "|  AAPL|2024-01-04 00:00:00|     -0.0127|\n",
      "|  AAPL|2024-01-05 00:00:00|      -0.004|\n",
      "|  AAPL|2024-01-08 00:00:00|      0.0242|\n",
      "|  AAPL|2024-01-09 00:00:00|     -0.0023|\n",
      "|  AAPL|2024-01-10 00:00:00|      0.0057|\n",
      "|  AAPL|2024-01-11 00:00:00|     -0.0032|\n",
      "|  AAPL|2024-01-12 00:00:00|      0.0018|\n",
      "|  AAPL|2024-01-16 00:00:00|     -0.0123|\n",
      "|  AAPL|2024-01-17 00:00:00|     -0.0052|\n",
      "|  AAPL|2024-01-18 00:00:00|      0.0326|\n",
      "|  AAPL|2024-01-19 00:00:00|      0.0155|\n",
      "|  AAPL|2024-01-22 00:00:00|      0.0122|\n",
      "|  AAPL|2024-01-23 00:00:00|      0.0067|\n",
      "|  AAPL|2024-01-24 00:00:00|     -0.0035|\n",
      "|  AAPL|2024-01-25 00:00:00|     -0.0017|\n",
      "|  AAPL|2024-01-26 00:00:00|      -0.009|\n",
      "|  AAPL|2024-01-29 00:00:00|     -0.0036|\n",
      "|  AAPL|2024-01-30 00:00:00|     -0.0192|\n",
      "|  AAPL|2024-01-31 00:00:00|     -0.0194|\n",
      "+------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STOCKS\n",
    "spark.sql(\"\"\"\n",
    "   SELECT Symbol, Date, ROUND(daily_return, 4) AS daily_return \n",
    "   FROM stocks \n",
    "   WHERE daily_return IS NOT NULL \n",
    "   ORDER BY Symbol, Date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b53bd7-e1b3-4a4d-bee5-848d412d84c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+\n",
      "|coin_name|               date|daily_return|\n",
      "+---------+-------------------+------------+\n",
      "|      ADA|2024-01-02 00:00:00|     -0.0293|\n",
      "|      ADA|2024-01-03 00:00:00|     -0.0795|\n",
      "|      ADA|2024-01-04 00:00:00|      0.0237|\n",
      "|      ADA|2024-01-05 00:00:00|     -0.0493|\n",
      "|      ADA|2024-01-06 00:00:00|     -0.0354|\n",
      "|      ADA|2024-01-07 00:00:00|     -0.0543|\n",
      "|      ADA|2024-01-08 00:00:00|      0.0948|\n",
      "|      ADA|2024-01-09 00:00:00|     -0.0541|\n",
      "|      ADA|2024-01-10 00:00:00|      0.1056|\n",
      "|      ADA|2024-01-11 00:00:00|      0.0275|\n",
      "|      ADA|2024-01-12 00:00:00|     -0.0593|\n",
      "|      ADA|2024-01-13 00:00:00|      0.0029|\n",
      "|      ADA|2024-01-14 00:00:00|     -0.0446|\n",
      "|      ADA|2024-01-15 00:00:00|      0.0059|\n",
      "|      ADA|2024-01-16 00:00:00|      0.0145|\n",
      "|      ADA|2024-01-17 00:00:00|     -0.0139|\n",
      "|      ADA|2024-01-18 00:00:00|      -0.049|\n",
      "|      ADA|2024-01-19 00:00:00|       0.004|\n",
      "|      ADA|2024-01-20 00:00:00|       0.023|\n",
      "|      ADA|2024-01-21 00:00:00|     -0.0248|\n",
      "+---------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#CRYPTO\n",
    "spark.sql(\"\"\"\n",
    "   SELECT coin_name, timestamp AS date, ROUND(daily_return, 4) AS daily_return\n",
    "   FROM crypto\n",
    "   WHERE daily_return IS NOT NULL\n",
    "   ORDER BY coin_name, timestamp\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df947b4f-9b93-4feb-8e6a-8c555121ee75",
   "metadata": {},
   "source": [
    "#### Loading - 3.1 Daily Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443515a8-7ba6-4c3d-8adf-483791a321dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results saved locally at: /tmp/crypto_daily_return.parquet_dir\n",
      "Final parquet file path: /tmp/crypto_daily_return.parquet\n",
      "File uploaded successfully: s3://ucl-de/data/crypto_daily_return.parquet\n"
     ]
    }
   ],
   "source": [
    "query_to_s3 = \"\"\"\n",
    "SELECT coin_name, timestamp AS date, ROUND(daily_return, 4) AS daily_return\n",
    "FROM crypto\n",
    "WHERE daily_return IS NOT NULL\n",
    "ORDER BY coin_name, timestamp\n",
    "\"\"\"\n",
    "\n",
    "# Define the filename for storing crypto volume spike results as a Parquet file\n",
    "parquet_file_name = \"crypto_daily_return.parquet\"\n",
    "\n",
    "# Execute the provided SQL query, store results locally as Parquet,\n",
    "# and then upload the file to AWS S3\n",
    "execute_query_and_upload_to_s3(query_to_s3, parquet_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21bec3-1bb7-48fb-b218-d0b8fbfd0888",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 3.2 Best and Worst Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1eb79811-46d1-4ffa-a3b9-bac95cc5e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocks: Total Return (Open vs Close)\n",
    "df_stocks = df_stocks.withColumn(\n",
    "    \"total_return\",\n",
    "    (col(\"Close\") - col(\"Open\")) / col(\"Open\")\n",
    ")\n",
    "\n",
    "# Crypto: Total Return (Open vs Close)\n",
    "df_crypto = df_crypto.withColumn(\n",
    "    \"total_return\",\n",
    "    (col(\"close\") - col(\"open\")) / col(\"open\")\n",
    ")\n",
    "\n",
    "df_stocks.createOrReplaceTempView(\"stocks\")\n",
    "df_crypto.createOrReplaceTempView(\"crypto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9f4a7fa-5c59-44c4-a37d-81b04dbe08e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|asset|return| type|\n",
      "+-----+------+-----+\n",
      "| AVGO|0.0962|Stock|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|asset|return|  type|\n",
      "+-----+------+------+\n",
      "|  TRX|0.9589|Crypto|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Stocks: Best Performer\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Symbol AS asset, ROUND(total_return, 4) AS return, 'Stock' AS type\n",
    "    FROM stocks\n",
    "    ORDER BY return DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()\n",
    "# Crypto: Best Performer\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name AS asset, ROUND(total_return, 4) AS return, 'Crypto' AS type\n",
    "    FROM crypto\n",
    "    ORDER BY return DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe6edaa-4d10-4415-8fb9-2367dc9eb921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|asset|return| type|\n",
      "+-----+------+-----+\n",
      "| TSLA|-0.094|Stock|\n",
      "+-----+------+-----+\n",
      "\n",
      "+-----+-------+------+\n",
      "|asset| return|  type|\n",
      "+-----+-------+------+\n",
      "|  TRX|-0.2474|Crypto|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Stocks: Worst Performer\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Symbol AS asset, ROUND(total_return, 4) AS return, 'Stock' AS type\n",
    "    FROM stocks\n",
    "    ORDER BY return ASC\n",
    "    LIMIT 1\n",
    "\"\"\").show()\n",
    "\n",
    "# Crypto: Worst Performer\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name AS asset, ROUND(total_return, 4) AS return, 'Crypto' AS type\n",
    "    FROM crypto\n",
    "    ORDER BY return ASC\n",
    "    LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14cff2-f9f0-44b8-a252-7bebd30f8a6c",
   "metadata": {},
   "source": [
    "#### 3.3 Custom Period Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "377b7c1c-ac0c-4bca-959b-0dddfcf17c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocks: First & Last Close Return\n",
    "window_period_stock = Window.partitionBy(\"Symbol\").orderBy(\"Date\")\n",
    "df_stocks = df_stocks.withColumn(\"first_close\", first(\"Close\").over(window_period_stock)) \\\n",
    "                     .withColumn(\"last_close\", last(\"Close\").over(window_period_stock)) \\\n",
    "                     .withColumn(\"total_return_custom\", (col(\"last_close\") - col(\"first_close\")) / col(\"first_close\"))\n",
    "\n",
    "# Crypto: First & Last Close Return\n",
    "window_period_crypto = Window.partitionBy(\"coin_name\").orderBy(\"timestamp\")\n",
    "df_crypto = df_crypto.withColumn(\"first_close\", first(\"close\").over(window_period_crypto)) \\\n",
    "                     .withColumn(\"last_close\", last(\"close\").over(window_period_crypto)) \\\n",
    "                     .withColumn(\"total_return_custom\", (col(\"last_close\") - col(\"first_close\")) / col(\"first_close\"))\n",
    "\n",
    "df_stocks.createOrReplaceTempView(\"stocks\")\n",
    "df_crypto.createOrReplaceTempView(\"crypto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1145f4fb-1154-4948-84b8-867c1712facb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|asset|return| type|\n",
      "+-----+------+-----+\n",
      "| NVDA|2.0915|Stock|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|asset|return|  type|\n",
      "+-----+------+------+\n",
      "| DOGE|4.0736|Crypto|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Top performers (Stock + Crypto)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Symbol AS asset, ROUND(total_return_custom, 4) AS return, 'Stock' AS type\n",
    "    FROM stocks\n",
    "    GROUP BY Symbol, total_return_custom\n",
    "    ORDER BY return DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name AS asset, ROUND(total_return_custom, 4) AS return, 'Crypto' AS type\n",
    "    FROM crypto\n",
    "    GROUP BY coin_name, total_return_custom\n",
    "    ORDER BY return DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61421a-0d7f-463c-8897-836604efaa42",
   "metadata": {},
   "source": [
    "#### 3.4 Weekly return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7deb8400-04f1-4478-815d-78128176ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocks\n",
    "df_stocks = df_stocks.withColumn(\"week\", weekofyear(\"Date\"))\n",
    "# Crypto\n",
    "df_crypto = df_crypto.withColumn(\"week\", weekofyear(\"timestamp\"))\n",
    "\n",
    "df_stocks.createOrReplaceTempView(\"stocks\")\n",
    "df_crypto.createOrReplaceTempView(\"crypto\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b61e3675-df68-4a04-8aa9-ba213b96806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------+-----+\n",
      "|asset|week|weekly_return| type|\n",
      "+-----+----+-------------+-----+\n",
      "| AAPL|   1|      -0.0094|Stock|\n",
      "| AAPL|   2|       0.0052|Stock|\n",
      "| AAPL|   3|       0.0077|Stock|\n",
      "| AAPL|   4|       9.0E-4|Stock|\n",
      "| AAPL|   5|      -0.0069|Stock|\n",
      "| AAPL|   6|       0.0035|Stock|\n",
      "| AAPL|   7|       -0.007|Stock|\n",
      "| AAPL|   8|       3.0E-4|Stock|\n",
      "| AAPL|   9|      -0.0031|Stock|\n",
      "| AAPL|  10|        -0.01|Stock|\n",
      "| AAPL|  11|       0.0022|Stock|\n",
      "| AAPL|  12|      -2.0E-4|Stock|\n",
      "| AAPL|  13|      -0.0011|Stock|\n",
      "| AAPL|  14|      -0.0022|Stock|\n",
      "| AAPL|  15|       0.0083|Stock|\n",
      "| AAPL|  16|      -0.0134|Stock|\n",
      "| AAPL|  17|       0.0052|Stock|\n",
      "| AAPL|  18|       0.0165|Stock|\n",
      "| AAPL|  19|      -1.0E-4|Stock|\n",
      "| AAPL|  20|       0.0074|Stock|\n",
      "+-----+----+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------+------+\n",
      "|asset|week|weekly_return|  type|\n",
      "+-----+----+-------------+------+\n",
      "|  ADA|   1|      -0.0039|Crypto|\n",
      "|  ADA|   2|       0.0015|Crypto|\n",
      "|  ADA|   3|      -8.0E-4|Crypto|\n",
      "|  ADA|   4|      -5.0E-4|Crypto|\n",
      "|  ADA|   5|       2.0E-4|Crypto|\n",
      "|  ADA|   6|        0.002|Crypto|\n",
      "|  ADA|   7|       0.0028|Crypto|\n",
      "|  ADA|   8|      -9.0E-4|Crypto|\n",
      "|  ADA|   9|       0.0044|Crypto|\n",
      "|  ADA|  10|      -1.0E-4|Crypto|\n",
      "|  ADA|  11|      -9.0E-4|Crypto|\n",
      "|  ADA|  12|      -8.0E-4|Crypto|\n",
      "|  ADA|  13|       1.0E-4|Crypto|\n",
      "|  ADA|  14|      -0.0019|Crypto|\n",
      "|  ADA|  15|      -0.0043|Crypto|\n",
      "|  ADA|  16|       0.0014|Crypto|\n",
      "|  ADA|  17|      -0.0016|Crypto|\n",
      "|  ADA|  18|      -1.0E-4|Crypto|\n",
      "|  ADA|  19|      -9.0E-4|Crypto|\n",
      "|  ADA|  20|       0.0014|Crypto|\n",
      "+-----+----+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Stocks: Weekly return\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Symbol AS asset, week, ROUND(AVG(daily_return), 4) AS weekly_return, 'Stock' AS type\n",
    "    FROM stocks\n",
    "    GROUP BY Symbol, week\n",
    "    ORDER BY Symbol, week\n",
    "\"\"\").show()\n",
    "\n",
    "# Crypto: Weekly return\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name AS asset, week, ROUND(AVG(daily_return), 4) AS weekly_return, 'Crypto' AS type\n",
    "    FROM crypto\n",
    "    GROUP BY coin_name, week\n",
    "    ORDER BY coin_name, week\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc8b25-61c0-4743-a7a6-6d733caac367",
   "metadata": {},
   "source": [
    "**Performance Metrics Summary**\n",
    "\n",
    "This section captures how individual stock and crypto assets performed over time, using return-based metrics to assess both consistency and extremes.\n",
    "\n",
    "**Daily Return**\n",
    "- **Stocks** like **AAPL** showed expected day-to-day fluctuations, reflecting natural market cycles. Values range from small negative to moderate positive returns, highlighting both correction and growth phases.\n",
    "- **Crypto** assets (e.g. **ADA**) showed repeated zero returns, suggesting data granularity issues or periods of low activity â€” though some days reflect sharp price drops or gains, emphasizing cryptoâ€™s jumpy nature.\n",
    "\n",
    "**Best & Worst Performing Assets**\n",
    "- Over the open-to-close daily window:\n",
    "  - **Top stock performer:** **AVGO**, signaling strong intraday growth.\n",
    "  - **Top crypto performer:** **TRX**, showing potential for explosive short-term gains.\n",
    "  - **Worst stock performer:** **TSLA**, possibly tied to a bearish phase or correction.\n",
    "  - **Worst crypto performer:** Also **TRX**, reinforcing the volatility theme in crypto.\n",
    "- Crypto assets can swing from top to bottom across different periods â€” unlike stocks, which tend to be more stable.\n",
    "\n",
    "**Custom Period Return (First to Last Close)**\n",
    "- Over the full dataset timeline:\n",
    "  - **Top stock:** **NVDA**, with a return >2x â€” consistent with its strong 2024 rally.\n",
    "  - **Top crypto:** **DOGE**, boasting over 4x return â€” meme coins often outperform in bull cycles.\n",
    "- Stocks show solid growth; crypto shows outsized returns but with less predictability.\n",
    "\n",
    "**Weekly Return**\n",
    "- Weekly stock returns (e.g. **AAPL**) offer smoother insights than daily â€” still minor swings, but more readable trends.\n",
    "- Weekly crypto returns (e.g. **ADA**) range from mild to volatile, but average out better over time â€” suggesting improved stability when zoomed out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ebbeb-737b-43cd-8692-650f16058a3c",
   "metadata": {},
   "source": [
    "### 4. Volatility & Risk\n",
    "#### 4.1 Rolling 7-day volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f96f769-16ea-4adb-84c1-a5e5a7e0b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from pyspark.sql.functions import stddev, avg, col,year\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Rolling 7-Day Volatility\n",
    "\n",
    "# Stock window\n",
    "rolling_window_stock = Window.partitionBy(\"Symbol\").orderBy(\"Date\").rowsBetween(-6, 0)\n",
    "df_stocks = df_stocks.withColumn(\"7_day_volatility\", stddev(\"daily_return\").over(rolling_window_stock))\n",
    "df_stocks.createOrReplaceTempView(\"stocks\")\n",
    "\n",
    "# Crypto window\n",
    "rolling_window_crypto = Window.partitionBy(\"coin_name\").orderBy(\"timestamp\").rowsBetween(-6, 0)\n",
    "df_crypto = df_crypto.withColumn(\"7_day_volatility\", stddev(\"daily_return\").over(rolling_window_crypto))\n",
    "df_crypto.createOrReplaceTempView(\"crypto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b85ed96d-eec1-42d8-847c-336845807293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+\n",
      "|Symbol|               Date|rolling_volatility|\n",
      "+------+-------------------+------------------+\n",
      "|  AAPL|2024-01-04 00:00:00|            0.0037|\n",
      "|  AAPL|2024-01-05 00:00:00|            0.0044|\n",
      "|  AAPL|2024-01-08 00:00:00|            0.0165|\n",
      "|  AAPL|2024-01-09 00:00:00|            0.0143|\n",
      "|  AAPL|2024-01-10 00:00:00|            0.0131|\n",
      "|  AAPL|2024-01-11 00:00:00|             0.012|\n",
      "|  AAPL|2024-01-12 00:00:00|            0.0115|\n",
      "|  AAPL|2024-01-16 00:00:00|            0.0115|\n",
      "|  AAPL|2024-01-17 00:00:00|            0.0116|\n",
      "|  AAPL|2024-01-18 00:00:00|            0.0144|\n",
      "|  AAPL|2024-01-19 00:00:00|             0.015|\n",
      "|  AAPL|2024-01-22 00:00:00|            0.0153|\n",
      "|  AAPL|2024-01-23 00:00:00|            0.0147|\n",
      "|  AAPL|2024-01-24 00:00:00|            0.0152|\n",
      "|  AAPL|2024-01-25 00:00:00|            0.0134|\n",
      "|  AAPL|2024-01-26 00:00:00|            0.0141|\n",
      "|  AAPL|2024-01-29 00:00:00|            0.0092|\n",
      "|  AAPL|2024-01-30 00:00:00|            0.0102|\n",
      "|  AAPL|2024-01-31 00:00:00|            0.0095|\n",
      "|  AAPL|2024-02-01 00:00:00|            0.0113|\n",
      "+------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query rolling volatility (Stocks)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Symbol, Date, ROUND(7_day_volatility, 4) AS rolling_volatility\n",
    "    FROM stocks\n",
    "    WHERE 7_day_volatility IS NOT NULL\n",
    "    ORDER BY Symbol, Date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8172c1a9-da4c-4990-ad3b-1f69042ad8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------------+\n",
      "|coin_name|               date|rolling_volatility|\n",
      "+---------+-------------------+------------------+\n",
      "|      ADA|2024-01-03 00:00:00|            0.0354|\n",
      "|      ADA|2024-01-04 00:00:00|            0.0516|\n",
      "|      ADA|2024-01-05 00:00:00|            0.0434|\n",
      "|      ADA|2024-01-06 00:00:00|            0.0376|\n",
      "|      ADA|2024-01-07 00:00:00|            0.0346|\n",
      "|      ADA|2024-01-08 00:00:00|            0.0591|\n",
      "|      ADA|2024-01-09 00:00:00|            0.0606|\n",
      "|      ADA|2024-01-10 00:00:00|            0.0709|\n",
      "|      ADA|2024-01-11 00:00:00|             0.071|\n",
      "|      ADA|2024-01-12 00:00:00|            0.0724|\n",
      "|      ADA|2024-01-13 00:00:00|            0.0704|\n",
      "|      ADA|2024-01-14 00:00:00|             0.069|\n",
      "|      ADA|2024-01-15 00:00:00|            0.0582|\n",
      "|      ADA|2024-01-16 00:00:00|            0.0537|\n",
      "|      ADA|2024-01-17 00:00:00|            0.0318|\n",
      "|      ADA|2024-01-18 00:00:00|              0.03|\n",
      "|      ADA|2024-01-19 00:00:00|            0.0256|\n",
      "|      ADA|2024-01-20 00:00:00|            0.0284|\n",
      "|      ADA|2024-01-21 00:00:00|            0.0251|\n",
      "|      ADA|2024-01-22 00:00:00|            0.0291|\n",
      "+---------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Query rolling volatility (Crypto)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name, timestamp AS date, ROUND(7_day_volatility, 4) AS rolling_volatility\n",
    "    FROM crypto\n",
    "    WHERE 7_day_volatility IS NOT NULL\n",
    "    ORDER BY coin_name, date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efd524-212b-4612-93e0-e957b758a289",
   "metadata": {},
   "source": [
    "#### Loading - 4.1 Rolling 7-day volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92b7b282-239a-4a13-b560-bfccc2cd5a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results saved locally at: /tmp/crypto_volatility.parquet_dir\n",
      "Final parquet file path: /tmp/crypto_volatility.parquet\n",
      "File uploaded successfully: s3://ucl-de/data/crypto_volatility.parquet\n"
     ]
    }
   ],
   "source": [
    "query_to_s3 = \"\"\"\n",
    "SELECT coin_name, timestamp AS date, ROUND(7_day_volatility, 4) AS rolling_volatility\n",
    "FROM crypto\n",
    "WHERE 7_day_volatility IS NOT NULL\n",
    "ORDER BY coin_name, date\n",
    "\"\"\"\n",
    "\n",
    "# Define the filename for storing crypto volume spike results as a Parquet file\n",
    "parquet_file_name = \"crypto_volatility.parquet\"\n",
    "\n",
    "# Execute the provided SQL query, store results locally as Parquet,\n",
    "# and then upload the file to AWS S3\n",
    "execute_query_and_upload_to_s3(query_to_s3, parquet_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce60c6-716d-4784-a6c0-ca187a256418",
   "metadata": {},
   "source": [
    "#### 4.2 Sharpe Ratio Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ce8efd4-5bbf-47f8-9b31-0ad07aab73a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+------------+\n",
      "|Symbol|avg_ret|volatility|sharpe_ratio|\n",
      "+------+-------+----------+------------+\n",
      "|  TSLA| 0.0029|    0.0401|       -0.43|\n",
      "|  NVDA| 0.0047|    0.0331|       -0.46|\n",
      "|  AVGO| 0.0037|     0.034|       -0.48|\n",
      "|  META| 0.0024|     0.023|       -0.76|\n",
      "|  AMZN| 0.0017|    0.0177|       -1.03|\n",
      "| GOOGL| 0.0015|    0.0177|       -1.05|\n",
      "|  GOOG| 0.0015|    0.0175|       -1.06|\n",
      "|  AAPL| 0.0013|    0.0141|       -1.32|\n",
      "|  MSFT| 7.0E-4|    0.0126|       -1.54|\n",
      "| BRK-B| 9.0E-4|    0.0092|       -2.08|\n",
      "|   SPY|  0.001|    0.0079|        -2.4|\n",
      "+------+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stocks\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Symbol,\n",
    "           ROUND(AVG(daily_return), 4) AS avg_ret,\n",
    "           ROUND(STDDEV(daily_return), 4) AS volatility,\n",
    "           ROUND((AVG(daily_return) - 0.02) / STDDEV(daily_return), 2) AS sharpe_ratio\n",
    "    FROM stocks\n",
    "    GROUP BY Symbol\n",
    "    ORDER BY sharpe_ratio DESC\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7c6bdbb-9db8-475f-89f2-b764637a3c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+------------+\n",
      "|coin_name|avg_ret|volatility|sharpe_ratio|\n",
      "+---------+-------+----------+------------+\n",
      "|      DOT| 3.0E-4|    0.0444|       -0.44|\n",
      "|      XLM| 0.0019|    0.0395|       -0.46|\n",
      "|      TRX| 4.0E-4|      0.02|       -0.98|\n",
      "|      ADA| 3.0E-4|    0.0167|       -1.18|\n",
      "|     DOGE| 3.0E-4|    0.0139|       -1.41|\n",
      "|     AVAX| 1.0E-4|    0.0132|       -1.51|\n",
      "|      XRP| 3.0E-4|    0.0111|       -1.78|\n",
      "|      SOL| 1.0E-4|    0.0104|        -1.9|\n",
      "|      ETH| 1.0E-4|    0.0088|       -2.27|\n",
      "|      BTC| 2.0E-4|     0.007|       -2.85|\n",
      "+---------+-------+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Crypto\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name,\n",
    "           ROUND(AVG(daily_return), 4) AS avg_ret,\n",
    "           ROUND(STDDEV(daily_return), 4) AS volatility,\n",
    "           ROUND((AVG(daily_return) - 0.02) / STDDEV(daily_return), 2) AS sharpe_ratio\n",
    "    FROM crypto\n",
    "    GROUP BY coin_name\n",
    "    ORDER BY sharpe_ratio DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ed456-e280-426e-9e93-20ca076ee096",
   "metadata": {},
   "source": [
    "**Volatility & Risk Summary**\n",
    "\n",
    "This section explores risk dynamics across stocks and crypto assets using **rolling volatility** and the **Sharpe ratio**, offering a cross-asset lens on return consistency and volatility exposure.\n",
    "\n",
    "**Rolling 7-Day Volatility**\n",
    "- Among stocks, **AAPL**, **META**, and **GOOG** exhibited rolling volatility ranging between **1%â€“1.6%**, showing relatively stable yet noticeable intraday fluctuations.\n",
    "- In crypto, **ADA** and **TRX** demonstrated much higher volatility bursts â€” crossing **3%+** on several occasions, reinforcing the market's **short-term unpredictability** and sensitivity to news or volume surges.\n",
    "- Rolling volatility effectively captures **near-term market choppiness**, helping identify **risk windows** and timing entry/exit points for trades.\n",
    "\n",
    "**Sharpe Ratio (Risk-Adjusted Return)**\n",
    "- Stock assets like **TSLA**, **AVGO**, and **NVDA** led with relatively higher (but still **negative**) Sharpe ratios, hinting at **suboptimal risk-adjusted returns** across the board in 2024.\n",
    "- Crypto Sharpe ratios were notably worse: **BTC**, **ETH**, and **DOGE** posted values as low as **-2.85**, **-2.27**, and **-1.41**, reflecting poor reward for the high volatility endured.\n",
    "- These figures suggest that while crypto may offer **explosive returns**, they rarely compensate for the corresponding **risk volatility**, especially when benchmarked to a 2% baseline return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01f3f8-ca30-4656-a09b-f08c6e97218e",
   "metadata": {},
   "source": [
    "### 5. Temporal Trends & Patterns\n",
    "#### 5.1 Quarterly average price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c6dd7c4-157e-4453-985e-32b44b6f657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------------------+\n",
      "|            quarter|Symbol|avg_quarterly_price|\n",
      "+-------------------+------+-------------------+\n",
      "|2024-01-01 00:00:00|  AAPL|             180.87|\n",
      "|2024-01-01 00:00:00|  AMZN|             166.93|\n",
      "|2024-01-01 00:00:00|  AVGO|             122.32|\n",
      "|2024-01-01 00:00:00| BRK-B|             393.34|\n",
      "|2024-01-01 00:00:00|  GOOG|             143.83|\n",
      "|2024-01-01 00:00:00| GOOGL|             142.54|\n",
      "|2024-01-01 00:00:00|  META|             444.61|\n",
      "|2024-01-01 00:00:00|  MSFT|              401.4|\n",
      "|2024-01-01 00:00:00|  NVDA|              72.46|\n",
      "|2024-01-01 00:00:00|   SPY|             491.81|\n",
      "|2024-01-01 00:00:00|  TSLA|             195.37|\n",
      "|2024-04-01 00:00:00|  AAPL|             185.75|\n",
      "|2024-04-01 00:00:00|  AMZN|              183.7|\n",
      "|2024-04-01 00:00:00|  AVGO|             138.91|\n",
      "|2024-04-01 00:00:00| BRK-B|             408.62|\n",
      "|2024-04-01 00:00:00|  GOOG|             169.54|\n",
      "|2024-04-01 00:00:00| GOOGL|             168.01|\n",
      "|2024-04-01 00:00:00|  META|             484.84|\n",
      "|2024-04-01 00:00:00|  MSFT|              419.5|\n",
      "|2024-04-01 00:00:00|  NVDA|             101.08|\n",
      "+-------------------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stocks\n",
    "spark.sql(\"\"\"\n",
    "    SELECT DATE_TRUNC('quarter', Date) AS quarter, Symbol, \n",
    "           ROUND(AVG(Close), 2) AS avg_quarterly_price\n",
    "    FROM stocks\n",
    "    GROUP BY quarter, Symbol\n",
    "    ORDER BY quarter, Symbol\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "846e5e51-8997-46bd-af56-f0e6d922cf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------------------+\n",
      "|            quarter|coin_name|avg_quarterly_price|\n",
      "+-------------------+---------+-------------------+\n",
      "|2024-01-01 00:00:00|      ADA|                0.6|\n",
      "|2024-01-01 00:00:00|     AVAX|              41.42|\n",
      "|2024-01-01 00:00:00|      BTC|            53574.8|\n",
      "|2024-01-01 00:00:00|     DOGE|               0.11|\n",
      "|2024-01-01 00:00:00|      DOT|               8.21|\n",
      "|2024-01-01 00:00:00|      ETH|            2920.36|\n",
      "|2024-01-01 00:00:00|      SOL|             123.64|\n",
      "|2024-01-01 00:00:00|      TRX|               0.12|\n",
      "|2024-01-01 00:00:00|      XLM|               0.12|\n",
      "|2024-01-01 00:00:00|      XRP|               0.58|\n",
      "|2024-04-01 00:00:00|      ADA|               0.46|\n",
      "|2024-04-01 00:00:00|     AVAX|              35.47|\n",
      "|2024-04-01 00:00:00|      BTC|           65679.27|\n",
      "|2024-04-01 00:00:00|     DOGE|               0.15|\n",
      "|2024-04-01 00:00:00|      DOT|               6.98|\n",
      "|2024-04-01 00:00:00|      ETH|            3372.51|\n",
      "|2024-04-01 00:00:00|      SOL|             154.72|\n",
      "|2024-04-01 00:00:00|      TRX|               0.12|\n",
      "|2024-04-01 00:00:00|      XLM|               0.11|\n",
      "|2024-04-01 00:00:00|      XRP|               0.52|\n",
      "+-------------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Crypto\n",
    "spark.sql(\"\"\"\n",
    "    SELECT DATE_TRUNC('quarter', timestamp) AS quarter, coin_name,\n",
    "           ROUND(AVG(close), 2) AS avg_quarterly_price\n",
    "    FROM crypto\n",
    "    GROUP BY quarter, coin_name\n",
    "    ORDER BY quarter, coin_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7233f0-b08e-46c6-8f18-ccd83fad18f5",
   "metadata": {},
   "source": [
    "#### 5.2 Yearly performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28001035-ed64-4500-baf7-94061ea0d22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|year|avg_yearly_return|\n",
      "+----+-----------------+\n",
      "|2024|             0.03|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stocks\n",
    "spark.sql(\"\"\"\n",
    "    SELECT YEAR(Date) AS year, \n",
    "           ROUND(AVG((Close - Open)/Open)*100, 2) AS avg_yearly_return\n",
    "    FROM stocks\n",
    "    GROUP BY year\n",
    "    ORDER BY year DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e33b89b5-4f85-477a-b491-ddc085b21c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|year|avg_yearly_return|\n",
      "+----+-----------------+\n",
      "|2024|             0.28|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Crypto\n",
    "spark.sql(\"\"\"\n",
    "    SELECT YEAR(timestamp) AS year, \n",
    "           ROUND(AVG((close - open)/open)*100, 2) AS avg_yearly_return\n",
    "    FROM crypto\n",
    "    GROUP BY year\n",
    "    ORDER BY year DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6468c-ad39-42b2-9e3e-42cd48c40dab",
   "metadata": {},
   "source": [
    "#### 5.3 Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0257d6c-2b43-4fe3-8747-e421f1f95a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocks\n",
    "window_50_stock = Window.partitionBy(\"Symbol\").orderBy(\"Date\").rowsBetween(-49, 0)\n",
    "window_200_stock = Window.partitionBy(\"Symbol\").orderBy(\"Date\").rowsBetween(-199, 0)\n",
    "\n",
    "df_stocks = df_stocks.withColumn(\"SMA_50\", avg(\"Close\").over(window_50_stock)) \\\n",
    "                     .withColumn(\"SMA_200\", avg(\"Close\").over(window_200_stock))\n",
    "df_stocks.createOrReplaceTempView(\"stocks\")\n",
    "\n",
    "# Crypto\n",
    "window_50_crypto = Window.partitionBy(\"coin_name\").orderBy(\"timestamp\").rowsBetween(-49, 0)\n",
    "window_200_crypto = Window.partitionBy(\"coin_name\").orderBy(\"timestamp\").rowsBetween(-199, 0)\n",
    "\n",
    "df_crypto = df_crypto.withColumn(\"SMA_50\", avg(\"close\").over(window_50_crypto)) \\\n",
    "                     .withColumn(\"SMA_200\", avg(\"close\").over(window_200_crypto))\n",
    "df_crypto.createOrReplaceTempView(\"crypto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddbc6db3-ad05-4868-8ac2-e6bb868d9399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+-------+\n",
      "|Symbol|               Date|SMA_50|SMA_200|\n",
      "+------+-------------------+------+-------+\n",
      "|  AAPL|2024-01-02 00:00:00|184.53| 184.53|\n",
      "|  AAPL|2024-01-03 00:00:00|183.84| 183.84|\n",
      "|  AAPL|2024-01-04 00:00:00|182.84| 182.84|\n",
      "|  AAPL|2024-01-05 00:00:00|182.15| 182.15|\n",
      "|  AAPL|2024-01-08 00:00:00|182.61| 182.61|\n",
      "|  AAPL|2024-01-09 00:00:00|182.85| 182.85|\n",
      "|  AAPL|2024-01-10 00:00:00|183.17| 183.17|\n",
      "|  AAPL|2024-01-11 00:00:00|183.33| 183.33|\n",
      "|  AAPL|2024-01-12 00:00:00| 183.5|  183.5|\n",
      "|  AAPL|2024-01-16 00:00:00| 183.4|  183.4|\n",
      "|  AAPL|2024-01-17 00:00:00|183.24| 183.24|\n",
      "|  AAPL|2024-01-18 00:00:00|183.59| 183.59|\n",
      "|  AAPL|2024-01-19 00:00:00|184.12| 184.12|\n",
      "|  AAPL|2024-01-22 00:00:00|184.73| 184.73|\n",
      "|  AAPL|2024-01-23 00:00:00|185.35| 185.35|\n",
      "|  AAPL|2024-01-24 00:00:00|185.85| 185.85|\n",
      "|  AAPL|2024-01-25 00:00:00|186.27| 186.27|\n",
      "|  AAPL|2024-01-26 00:00:00|186.55| 186.55|\n",
      "|  AAPL|2024-01-29 00:00:00|186.76| 186.76|\n",
      "|  AAPL|2024-01-30 00:00:00|186.77| 186.77|\n",
      "+------+-------------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stocks\n",
    "spark.sql(\"\"\"\n",
    "   SELECT Symbol, Date, ROUND(SMA_50, 2) AS SMA_50, ROUND(SMA_200, 2) AS SMA_200 \n",
    "   FROM stocks \n",
    "   WHERE SMA_50 IS NOT NULL AND SMA_200 IS NOT NULL \n",
    "   ORDER BY Symbol, Date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71b5415d-a441-48f1-9e87-961268a99f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------+-------+\n",
      "|coin_name|               date|SMA_50|SMA_200|\n",
      "+---------+-------------------+------+-------+\n",
      "|      ADA|2024-01-01 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-01 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-01 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-01 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-01 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-01 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-01 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-02 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-02 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-02 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-02 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-02 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-02 00:00:00|  0.62|   0.62|\n",
      "|      ADA|2024-01-02 00:00:00|  0.61|   0.61|\n",
      "|      ADA|2024-01-03 00:00:00|  0.61|   0.61|\n",
      "|      ADA|2024-01-03 00:00:00|  0.61|   0.61|\n",
      "|      ADA|2024-01-03 00:00:00|   0.6|    0.6|\n",
      "|      ADA|2024-01-03 00:00:00|   0.6|    0.6|\n",
      "|      ADA|2024-01-03 00:00:00|   0.6|    0.6|\n",
      "|      ADA|2024-01-03 00:00:00|   0.6|    0.6|\n",
      "+---------+-------------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#crypto\n",
    "spark.sql(\"\"\"\n",
    "    SELECT coin_name, timestamp AS date, \n",
    "           ROUND(SMA_50, 2) AS SMA_50, ROUND(SMA_200, 2) AS SMA_200\n",
    "    FROM crypto\n",
    "    WHERE SMA_50 IS NOT NULL AND SMA_200 IS NOT NULL\n",
    "    ORDER BY coin_name, date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a634c72-3ea9-4c11-bb9f-b0836f0ea2a7",
   "metadata": {},
   "source": [
    "**Temporal Trends & Patterns Summary**\n",
    "\n",
    "This section examines how stock and crypto assets evolve across different time horizons â€” quarterly, yearly, and with moving averages â€” to highlight seasonality, momentum, and macro-level patterns.\n",
    "\n",
    "**Quarterly Average Price**\n",
    "- In stocks, **META**, **SPY**, and **MSFT** exhibited consistently high quarterly averages â€” reflecting strength and stability across quarters.\n",
    "- For crypto, top-value assets like **BTC** and **ETH** posted the highest quarterly averages (e.g., **BTC > $65k in Q2**), while **ADA**, **XLM**, and **TRX** showed lower averages but potential for sharper relative gains due to their price sensitivity.\n",
    "\n",
    "**Yearly Performance (2024)**\n",
    "- **Crypto** outperformed stocks in 2024:  \n",
    "  - Stocks returned an average of **+0.03%**, indicating a flat year possibly due to broader market uncertainty.\n",
    "  - Cryptos surged with **+0.28%** on average â€” driven by recovery sentiment and speculative inflows.\n",
    "- This stark contrast underscores how market cycles can affect asset classes differently.\n",
    "\n",
    "**Moving Averages (SMA 50 vs. SMA 200)**\n",
    "- For stocks like **AAPL**, the **50-day SMA remained below the 200-day SMA** for extended periods â€” signaling caution or bearish trend continuation.\n",
    "- In crypto, moving averages for **ADA**, **XRP**, and **DOGE** were often flat or closely aligned, indicating low trend momentum or range-bound movement during certain periods.\n",
    "- These crossovers (or lack thereof) help flag potential trend reversals or confirm trend continuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c070a4-b320-4b95-92f1-6558e0f47cfe",
   "metadata": {},
   "source": [
    "### 6. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9374b865-fcae-45b1-b32d-08efb7af8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sentiment label\n",
    "from pyspark.sql.functions import when\n",
    "df_posts = df_posts.withColumn(\"sentiment_label\",\n",
    "    when(col(\"polarity\") > 0.1, \"Positive\")\n",
    "    .when(col(\"polarity\") < -0.1, \"Negative\")\n",
    "    .otherwise(\"Neutral\"))\n",
    "df_posts.createOrReplaceTempView(\"posts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61e59c6b-4a86-4d06-834a-426c153d5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for rolling sentiment\n",
    "rolling = Window.orderBy(\"timestamp\").rowsBetween(-6, 0)\n",
    "df_posts = df_posts.withColumn(\"rolling_sentiment\", avg(\"polarity\").over(rolling))\n",
    "df_posts.createOrReplaceTempView(\"posts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d114a54-a14f-493b-b0e3-ba782cf4631f",
   "metadata": {},
   "source": [
    "#### 6.1 Avg daily sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f151a49b-70aa-49ad-87e7-69aabd87800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|          timestamp|avg_daily_polarity|\n",
      "+-------------------+------------------+\n",
      "|               NULL|               0.0|\n",
      "|2010-10-19 00:00:00|            0.1292|\n",
      "|2010-10-21 00:00:00|               0.0|\n",
      "|2010-10-22 00:00:00|            0.2313|\n",
      "|2010-10-25 00:00:00|               0.0|\n",
      "|2010-10-26 00:00:00|            0.3143|\n",
      "|2010-10-27 00:00:00|              0.23|\n",
      "|2010-10-31 00:00:00|             0.119|\n",
      "|2013-04-02 00:00:00|            0.3575|\n",
      "|2013-04-04 00:00:00|            0.0406|\n",
      "|2013-04-18 00:00:00|               0.1|\n",
      "|2020-05-03 00:00:00|             0.188|\n",
      "|2020-05-13 00:00:00|            0.2069|\n",
      "|2020-06-12 00:00:00|            0.1014|\n",
      "|2020-07-08 00:00:00|               0.8|\n",
      "|2020-09-15 00:00:00|            0.1606|\n",
      "|2020-09-22 00:00:00|            -0.082|\n",
      "|2020-09-23 00:00:00|              0.34|\n",
      "|2020-12-03 00:00:00|             0.112|\n",
      "|2022-12-22 00:00:00|           -0.0712|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT timestamp, ROUND(AVG(polarity), 4) AS avg_daily_polarity \n",
    "   FROM posts \n",
    "   GROUP BY timestamp \n",
    "   ORDER BY timestamp\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3be49-c16f-460d-8993-c67000677d1c",
   "metadata": {},
   "source": [
    "#### 6.2 Sentiment by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ad737b4-efa0-4446-a129-b4b15dc0de3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------+\n",
      "|          timestamp|subsection|       avg_polarity|\n",
      "+-------------------+----------+-------------------+\n",
      "|2024-01-02 00:00:00|       BTC|     0.169642572845|\n",
      "|2024-12-24 00:00:00|       BTC|      0.07767320245|\n",
      "|2024-10-28 00:00:00|      DOGE|0.05538033395714286|\n",
      "|2024-10-29 00:00:00|      DOGE|0.17081651805384615|\n",
      "|2024-11-06 00:00:00|       BTC|       0.1938449306|\n",
      "|2024-11-08 00:00:00|       BTC|0.11535687230000002|\n",
      "|2024-11-18 00:00:00|       XRP|       0.0732195037|\n",
      "|2024-07-23 00:00:00|       ETH|       0.0758122896|\n",
      "|2024-02-08 00:00:00|       BTC|     0.182236925225|\n",
      "|2020-12-03 00:00:00|       SOL|              0.112|\n",
      "|2024-06-04 00:00:00|       XRP|0.14836647725000002|\n",
      "|2010-10-19 00:00:00|       BTC|0.12916666666666668|\n",
      "|2024-01-03 00:00:00|       BTC| 0.1788957548111111|\n",
      "|2024-12-05 00:00:00|       TRX|0.09723930480000001|\n",
      "|2024-10-09 00:00:00|       BTC|      0.19213541665|\n",
      "|2024-03-20 00:00:00|       BTC|0.13948335129999997|\n",
      "|2024-06-09 00:00:00|       ETH|      0.08296693444|\n",
      "|2024-04-27 00:00:00|       BTC|     0.141957316675|\n",
      "|2024-02-01 00:00:00|       BTC|      -0.0277777778|\n",
      "|2024-06-11 00:00:00|       ETH|                0.2|\n",
      "+-------------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT timestamp, subsection, AVG(polarity) AS avg_polarity \n",
    "   FROM posts \n",
    "   GROUP BY timestamp, subsection\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13bc3b-7d5f-4010-aaba-2c4111a9f30e",
   "metadata": {},
   "source": [
    "#### 6.3 Positive & Negative posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5543ad66-0a33-43a5-b48d-3df913583beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------+\n",
      "|          timestamp|        cleaned_post|polarity|\n",
      "+-------------------+--------------------+--------+\n",
      "|2024-07-14 00:00:00|OnlyFans An excel...|     1.0|\n",
      "|2024-03-13 00:00:00|I agree to the ot...|     1.0|\n",
      "|2020-07-08 00:00:00|Hey guys $SOL is ...|     0.8|\n",
      "|2024-06-22 00:00:00|Can Bitcoin be a ...|     0.7|\n",
      "|2024-01-02 00:00:00|I dont know that ...|   0.625|\n",
      "+-------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT timestamp, cleaned_post, polarity \n",
    "   FROM posts \n",
    "   ORDER BY polarity \n",
    "   DESC LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "399705e6-a4e4-4164-8144-58b11e773b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|          timestamp|        cleaned_post|     polarity|\n",
      "+-------------------+--------------------+-------------+\n",
      "|2024-07-28 00:00:00|I agree, currency...|         -0.8|\n",
      "|2024-07-28 00:00:00|I agree, currency...|       -0.345|\n",
      "|2024-03-11 00:00:00|Kucoin o Coinbase...|-0.3333333333|\n",
      "|2024-03-15 00:00:00|What wallet are y...|-0.3083333333|\n",
      "|2010-10-19 00:00:00|-10BTC for forced...|         -0.3|\n",
      "+-------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT timestamp, cleaned_post, polarity \n",
    "   FROM posts \n",
    "   ORDER BY polarity \n",
    "   ASC LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064cfba6-3199-40c5-8fd6-a6e438ad45ba",
   "metadata": {},
   "source": [
    "#### 6.4 Sentiment label counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2595b88-7734-44ff-ad18-d43cf1e8b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|sentiment_label|count|\n",
      "+---------------+-----+\n",
      "|       Positive|  853|\n",
      "|        Neutral|  635|\n",
      "|       Negative|   38|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT sentiment_label, COUNT(*) AS count\n",
    "   FROM posts \n",
    "   GROUP BY sentiment_label\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3875b-9e66-47bf-8ec8-fb96ced1f682",
   "metadata": {},
   "source": [
    "#### 6.5  Sentiment volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c13113b-32c8-4183-8f4b-393877784d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+\n",
      "|          timestamp|       avg_polarity|post_count|\n",
      "+-------------------+-------------------+----------+\n",
      "|               NULL|                0.0|        94|\n",
      "|2010-10-19 00:00:00|0.12916666666666668|         3|\n",
      "|2010-10-21 00:00:00|                0.0|         1|\n",
      "|2010-10-22 00:00:00|            0.23125|         5|\n",
      "|2010-10-25 00:00:00|                0.0|         2|\n",
      "|2010-10-26 00:00:00|       0.3142857143|         1|\n",
      "|2010-10-27 00:00:00| 0.2299886621142857|         7|\n",
      "|2010-10-31 00:00:00|        0.119047619|         1|\n",
      "|2013-04-02 00:00:00|        0.357521645|         2|\n",
      "|2013-04-04 00:00:00|           0.040625|         1|\n",
      "|2013-04-18 00:00:00|                0.1|         1|\n",
      "|2020-05-03 00:00:00|0.18797348484999998|         6|\n",
      "|2020-05-13 00:00:00|0.20685714285999998|         5|\n",
      "|2020-06-12 00:00:00|      0.10139668365|         2|\n",
      "|2020-07-08 00:00:00|                0.8|         1|\n",
      "|2020-09-15 00:00:00|     0.160602678575|         4|\n",
      "|2020-09-22 00:00:00|             -0.082|         1|\n",
      "|2020-09-23 00:00:00|               0.34|         1|\n",
      "|2020-12-03 00:00:00|              0.112|         2|\n",
      "|2022-12-22 00:00:00|      -0.0711538462|         1|\n",
      "+-------------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT timestamp, AVG(polarity) AS avg_polarity, COUNT(post) AS post_count \n",
    "   FROM posts \n",
    "   GROUP BY timestamp \n",
    "   ORDER BY timestamp\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83bee1-735a-48a4-942a-3c181384111d",
   "metadata": {},
   "source": [
    "**Sentiment Analysis Summary**\n",
    "\n",
    "This section explores the tone and emotion behind user-generated content, uncovering how sentiment trends correlate with asset discussions over time.\n",
    "\n",
    "**Avg Daily Sentiment**\n",
    "- Overall average polarity fluctuated across the timeline, with noticeable **positive spikes** in 2013 and **negative dips** around late 2020 and early 2022.\n",
    "- Daily sentiment varies, indicating **market mood swings** likely tied to major events or price movements.\n",
    "\n",
    "**Sentiment by Topic**\n",
    "- **BTC** consistently appeared across timestamps with mild to moderately positive sentiment (e.g., Jan 2 and Nov 6).\n",
    "- Posts tagged under **DOGE**, **SOL**, and **TRX** also show mostly positive polarity, reaffirming strong community backing.\n",
    "- **ETH** appeared slightly neutral to positive, while **XRP** sentiment fluctuated.\n",
    "\n",
    "**Top Positive & Negative Posts**\n",
    "- The **most positive posts** expressed enthusiasm and optimism toward specific coins like $SOL and BTC adoption.\n",
    "- The **most negative posts** reflected strong skepticism or frustrations â€” often hinting at scams or exchange issues (e.g., Kucoin, forced withdrawal mentions).\n",
    "\n",
    "**Sentiment Label Distribution**\n",
    "- Most posts were **Positive (853)** or **Neutral (635)** â€” indicating a predominantly bullish or balanced tone in discussions.\n",
    "- Very few posts were **Negative (38)**, suggesting either a generally optimistic community or low representation of contrarian opinions.\n",
    "\n",
    "**Sentiment Volume**\n",
    "- Post volume was **heaviest during neutral sentiment days**, though high positivity also aligned with days of elevated activity.\n",
    "- Days like **2020-07-08** and **2020-05-13** saw extremely high polarity (+0.8, +0.26) paired with spikes in post count â€” likely around breakout discussions or news hype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f610f2f-7716-4c3c-b00a-646950d73472",
   "metadata": {},
   "source": [
    "### 7. Merging All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34d5ac67-7c5e-453e-af9e-084887b4f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "# Aggregate stock data\n",
    "stocks_prepped = df_stocks.groupBy(\"Date\").agg(\n",
    "    avg(\"Open\").alias(\"s_open\"),\n",
    "    avg(\"Close\").alias(\"s_close\")\n",
    ").withColumnRenamed(\"Date\", \"DATE\")\n",
    "\n",
    "# Aggregate crypto data\n",
    "crypto_prepped = df_crypto.groupBy(\"timestamp\").agg(\n",
    "    avg(\"open\").alias(\"c_open\"),\n",
    "    avg(\"close\").alias(\"c_close\")\n",
    ").withColumnRenamed(\"timestamp\", \"DATE\")\n",
    "\n",
    "# Aggregate sentiment data\n",
    "sentiment_prepped = df_posts.groupBy(\"timestamp\").agg(\n",
    "    avg(\"polarity\").alias(\"sentiment\")\n",
    ").withColumnRenamed(\"timestamp\", \"DATE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19b8d18a-d2f4-4234-8648-7913fafd8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all 3 on DATE\n",
    "transformed_data = stocks_prepped \\\n",
    "    .join(crypto_prepped, on=\"DATE\", how=\"inner\") \\\n",
    "    .join(sentiment_prepped, on=\"DATE\", how=\"inner\")\n",
    "\n",
    "transformed_data.createOrReplaceTempView(\"transformed_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d9278-38ea-4a8b-a7cf-8ba116492f97",
   "metadata": {},
   "source": [
    "### 8. Combined Analysis\n",
    "#### 8.1 Price Comparison Between Stocks & Crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c79bab47-cd7d-44c4-84eb-8b9157a9dee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+-----------------+\n",
      "|               DATE|stock_pct_change|crypto_pct_change|\n",
      "+-------------------+----------------+-----------------+\n",
      "|2024-01-02 00:00:00|           -0.32|             1.69|\n",
      "|2024-01-03 00:00:00|           -0.03|            -4.78|\n",
      "|2024-01-24 00:00:00|             0.0|             0.44|\n",
      "|2024-01-25 00:00:00|           -0.14|            -0.37|\n",
      "|2024-01-26 00:00:00|            0.03|             4.57|\n",
      "|2024-01-31 00:00:00|           -1.08|            -1.05|\n",
      "|2024-02-01 00:00:00|            0.63|             1.22|\n",
      "|2024-02-08 00:00:00|            0.09|             2.12|\n",
      "|2024-02-09 00:00:00|            0.72|             4.09|\n",
      "|2024-02-13 00:00:00|            0.21|            -0.45|\n",
      "|2024-02-14 00:00:00|            0.56|             4.28|\n",
      "|2024-02-26 00:00:00|            -0.9|             5.22|\n",
      "|2024-02-27 00:00:00|             0.1|             4.52|\n",
      "|2024-02-28 00:00:00|            0.03|             9.25|\n",
      "|2024-03-05 00:00:00|           -1.03|            -6.43|\n",
      "|2024-03-06 00:00:00|           -0.55|             3.84|\n",
      "|2024-03-07 00:00:00|            0.84|             1.27|\n",
      "|2024-03-11 00:00:00|           -0.23|             4.48|\n",
      "|2024-03-12 00:00:00|             0.8|            -0.92|\n",
      "|2024-03-13 00:00:00|           -0.24|             2.27|\n",
      "+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Daily Change Percentage (Stocks vs Crypto)\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "  DATE,\n",
    "  ROUND(((s_close - s_open)/s_open)*100, 2) AS stock_pct_change,\n",
    "  ROUND(((c_close - c_open)/c_open)*100, 2) AS crypto_pct_change\n",
    "FROM transformed_data\n",
    "ORDER BY DATE\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a33fcb-655c-48a1-99e0-0545961ae975",
   "metadata": {},
   "source": [
    "#### 8.2 Sentiment vs Daily Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46b4b52e-3d7c-461a-8c0f-14bcab85a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+-----------------+-------------+\n",
      "|               DATE|stock_pct_change|crypto_pct_change|avg_sentiment|\n",
      "+-------------------+----------------+-----------------+-------------+\n",
      "|2024-01-02 00:00:00|           -0.32|             1.69|         0.17|\n",
      "|2024-01-03 00:00:00|           -0.03|            -4.78|        0.179|\n",
      "|2024-01-24 00:00:00|             0.0|             0.44|        0.125|\n",
      "|2024-01-25 00:00:00|           -0.14|            -0.37|        0.204|\n",
      "|2024-01-26 00:00:00|            0.03|             4.57|        0.064|\n",
      "|2024-01-31 00:00:00|           -1.08|            -1.05|        0.097|\n",
      "|2024-02-01 00:00:00|            0.63|             1.22|       -0.028|\n",
      "|2024-02-08 00:00:00|            0.09|             2.12|        0.182|\n",
      "|2024-02-09 00:00:00|            0.72|             4.09|        0.235|\n",
      "|2024-02-13 00:00:00|            0.21|            -0.45|        0.174|\n",
      "|2024-02-14 00:00:00|            0.56|             4.28|        0.169|\n",
      "|2024-02-26 00:00:00|            -0.9|             5.22|       -0.121|\n",
      "|2024-02-27 00:00:00|             0.1|             4.52|        0.062|\n",
      "|2024-02-28 00:00:00|            0.03|             9.25|        0.148|\n",
      "|2024-03-05 00:00:00|           -1.03|            -6.43|        0.019|\n",
      "|2024-03-06 00:00:00|           -0.55|             3.84|        0.081|\n",
      "|2024-03-07 00:00:00|            0.84|             1.27|        0.159|\n",
      "|2024-03-11 00:00:00|           -0.23|             4.48|        0.128|\n",
      "|2024-03-12 00:00:00|             0.8|            -0.92|        0.233|\n",
      "|2024-03-13 00:00:00|           -0.24|             2.27|        0.323|\n",
      "+-------------------+----------------+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "  DATE,\n",
    "  ROUND(((s_close - s_open)/s_open)*100, 2) AS stock_pct_change,\n",
    "  ROUND(((c_close - c_open)/c_open)*100, 2) AS crypto_pct_change,\n",
    "  ROUND(sentiment, 3) AS avg_sentiment\n",
    "FROM transformed_data\n",
    "ORDER BY DATE\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5daaad-fb22-4d75-8ea0-48603cc01f9c",
   "metadata": {},
   "source": [
    "#### Loading - 8.2 Sentiment vs Daily Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "492e8fe5-5a9e-4cad-95df-054428e47993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results saved locally at: /tmp/sentiment_vs_daily_return.parquet_dir\n",
      "Final parquet file path: /tmp/sentiment_vs_daily_return.parquet\n",
      "File uploaded successfully: s3://ucl-de/data/sentiment_vs_daily_return.parquet\n"
     ]
    }
   ],
   "source": [
    "query_to_s3 = \"\"\"\n",
    "SELECT \n",
    "  DATE,\n",
    "  ROUND(((s_close - s_open)/s_open)*100, 2) AS stock_pct_change,\n",
    "  ROUND(((c_close - c_open)/c_open)*100, 2) AS crypto_pct_change,\n",
    "  ROUND(sentiment, 3) AS avg_sentiment\n",
    "FROM transformed_data\n",
    "ORDER BY DATE\n",
    "\"\"\"\n",
    "\n",
    "# Define the filename for storing crypto volume spike results as a Parquet file\n",
    "parquet_file_name = \"sentiment_vs_daily_return.parquet\"\n",
    "\n",
    "# Execute the provided SQL query, store results locally as Parquet,\n",
    "# and then upload the file to AWS S3\n",
    "execute_query_and_upload_to_s3(query_to_s3, parquet_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922966f-f3a5-416c-b0de-6dc1fa21648e",
   "metadata": {},
   "source": [
    "#### 8.3 Days When Sentiment Was High but Prices Dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a95c6220-2a39-44e1-b571-a2e13e03684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+----------------+-----------------+\n",
      "|               DATE|sentiment|stock_pct_change|crypto_pct_change|\n",
      "+-------------------+---------+----------------+-----------------+\n",
      "|2024-03-13 00:00:00|    0.323|           -0.24|             2.27|\n",
      "+-------------------+---------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "  DATE,\n",
    "  ROUND(sentiment, 3) AS sentiment,\n",
    "  ROUND(((s_close - s_open)/s_open)*100, 2) AS stock_pct_change,\n",
    "  ROUND(((c_close - c_open)/c_open)*100, 2) AS crypto_pct_change\n",
    "FROM transformed_data\n",
    "WHERE sentiment > 0.3 \n",
    "  AND (((s_close - s_open)/s_open) < 0 OR ((c_close - c_open)/c_open) < 0)\n",
    "ORDER BY DATE\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b07e7-1f09-4acc-8e21-27692dc30757",
   "metadata": {},
   "source": [
    "#### Loading - 8.3 Days When Sentiment Was High but Prices Dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c931298-98d7-4fe3-8871-fd7d0b375ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results saved locally at: /tmp/sentiment_high_prices_low.parquet_dir\n",
      "Final parquet file path: /tmp/sentiment_high_prices_low.parquet\n",
      "File uploaded successfully: s3://ucl-de/data/sentiment_high_prices_low.parquet\n"
     ]
    }
   ],
   "source": [
    "query_to_s3 = \"\"\"\n",
    "SELECT \n",
    "  DATE,\n",
    "  ROUND(sentiment, 3) AS sentiment,\n",
    "  ROUND(((s_close - s_open)/s_open)*100, 2) AS stock_pct_change,\n",
    "  ROUND(((c_close - c_open)/c_open)*100, 2) AS crypto_pct_change\n",
    "FROM transformed_data\n",
    "WHERE sentiment > 0.3 \n",
    "  AND (((s_close - s_open)/s_open) < 0 OR ((c_close - c_open)/c_open) < 0)\n",
    "ORDER BY DATE\n",
    "\"\"\"\n",
    "\n",
    "# Define the filename for storing crypto volume spike results as a Parquet file\n",
    "parquet_file_name = \"sentiment_high_prices_low.parquet\"\n",
    "\n",
    "# Execute the provided SQL query, store results locally as Parquet,\n",
    "# and then upload the file to AWS S3\n",
    "execute_query_and_upload_to_s3(query_to_s3, parquet_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c4c1a-557d-44b9-b297-eb3a477b0450",
   "metadata": {},
   "source": [
    "#### 8.4 Days When Sentiment Was Low but Prices Rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32ca6581-b798-4f0a-bec7-42c2488d9441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+----------------+-----------------+\n",
      "|               DATE|sentiment|stock_pct_change|crypto_pct_change|\n",
      "+-------------------+---------+----------------+-----------------+\n",
      "|2024-02-01 00:00:00|   -0.028|            0.63|             1.22|\n",
      "|2024-02-26 00:00:00|   -0.121|            -0.9|             5.24|\n",
      "|2024-12-16 00:00:00|    0.025|            1.22|             1.62|\n",
      "+-------------------+---------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATE,\n",
    "    ROUND(sentiment, 3) AS sentiment,\n",
    "    ROUND(((s_close - s_open)/s_open)*100, 2) AS stock_pct_change,\n",
    "    ROUND(((c_close - c_open)/c_open)*100, 2) AS crypto_pct_change\n",
    "FROM transformed_data\n",
    "WHERE sentiment < 0.05\n",
    "  AND (((s_close - s_open)/s_open) > 0 OR ((c_close - c_open)/c_open) > 0)\n",
    "ORDER BY DATE\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10b24c-92d8-422e-98f4-2e170e3d53e8",
   "metadata": {},
   "source": [
    "#### Loading - 8.4 Days When Sentiment Was Low but Prices Rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db58d062-1f23-4bd7-8b6d-c62f7654edaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results saved locally at: /tmp/sentiment_low_prices_high.parquet_dir\n",
      "Final parquet file path: /tmp/sentiment_low_prices_high.parquet\n",
      "File uploaded successfully: s3://ucl-de/data/sentiment_low_prices_high.parquet\n"
     ]
    }
   ],
   "source": [
    "query_to_s3 = \"\"\"\n",
    "SELECT\n",
    "    DATE,\n",
    "    ROUND(sentiment, 3) AS sentiment,\n",
    "    ROUND(((s_close - s_open)/s_open)*100, 2) AS stock_pct_change,\n",
    "    ROUND(((c_close - c_open)/c_open)*100, 2) AS crypto_pct_change\n",
    "FROM transformed_data\n",
    "WHERE sentiment < 0.05\n",
    "  AND (((s_close - s_open)/s_open) > 0 OR ((c_close - c_open)/c_open) > 0)\n",
    "ORDER BY DATE\n",
    "\"\"\"\n",
    "\n",
    "# Define the filename for storing crypto volume spike results as a Parquet file\n",
    "parquet_file_name = \"sentiment_low_prices_high.parquet\"\n",
    "\n",
    "# Execute the provided SQL query, store results locally as Parquet,\n",
    "# and then upload the file to AWS S3\n",
    "execute_query_and_upload_to_s3(query_to_s3, parquet_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158a43b-967d-4b0d-a87b-01406b027044",
   "metadata": {},
   "source": [
    "#### 8.5 Weekly Avg Sentiment and Market Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92f139aa-2892-4d9f-a3ff-dff53ca1eb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+-----------------+\n",
      "|week_num|avg_weekly_sentiment|avg_stock_return|avg_crypto_return|\n",
      "+--------+--------------------+----------------+-----------------+\n",
      "|       1|                0.15|           -0.07|            -1.34|\n",
      "|       4|               0.131|           -0.04|             1.55|\n",
      "|       5|               0.035|           -0.23|             0.09|\n",
      "|       6|               0.208|            0.41|             3.11|\n",
      "|       7|               0.171|            0.39|             1.92|\n",
      "|       9|               0.029|           -0.26|             6.35|\n",
      "|      10|               0.087|           -0.25|            -0.45|\n",
      "|      11|               0.158|           -0.06|             0.16|\n",
      "|      12|                 0.1|            0.36|            -1.26|\n",
      "|      13|               0.124|           -0.24|             0.54|\n",
      "|      44|               0.124|           -0.66|             1.28|\n",
      "|      45|               0.136|            0.96|             3.23|\n",
      "|      46|               0.107|           -0.81|             0.34|\n",
      "|      47|               0.073|            0.12|             0.86|\n",
      "|      49|                0.17|            0.63|             1.08|\n",
      "|      50|               0.113|            0.17|             1.37|\n",
      "|      51|               0.025|            1.22|             1.62|\n",
      "|      52|               0.089|            1.05|             1.93|\n",
      "+--------+--------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "  WEEKOFYEAR(DATE) AS week_num,\n",
    "  ROUND(AVG(sentiment), 3) AS avg_weekly_sentiment,\n",
    "  ROUND(AVG((s_close - s_open)/s_open)*100, 2) AS avg_stock_return,\n",
    "  ROUND(AVG((c_close - c_open)/c_open)*100, 2) AS avg_crypto_return\n",
    "FROM transformed_data\n",
    "GROUP BY WEEKOFYEAR(DATE)\n",
    "ORDER BY week_num\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825ed1c-208c-427b-92c2-36fe222b4c63",
   "metadata": {},
   "source": [
    "#### 8.6 Lagged Sentiment vs Future Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "126b5e13-a707-4597-bcf6-2eb622e1c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 17:26:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------------+----------------------+\n",
      "|               DATE|sentiment|next_day_stock_return|next_day_crypto_return|\n",
      "+-------------------+---------+---------------------+----------------------+\n",
      "|2024-01-02 00:00:00|     0.17|                  0.0|                  -5.0|\n",
      "|2024-01-03 00:00:00|    0.179|                  0.0|                   0.0|\n",
      "|2024-01-24 00:00:00|    0.125|                  0.0|                   0.0|\n",
      "|2024-01-25 00:00:00|    0.204|                  0.0|                   5.0|\n",
      "|2024-01-26 00:00:00|    0.064|                 -1.0|                  -1.0|\n",
      "|2024-01-31 00:00:00|    0.097|                  1.0|                   1.0|\n",
      "|2024-02-01 00:00:00|   -0.028|                  0.0|                   2.0|\n",
      "|2024-02-08 00:00:00|    0.182|                  1.0|                   4.0|\n",
      "|2024-02-09 00:00:00|    0.235|                  0.0|                   0.0|\n",
      "|2024-02-13 00:00:00|    0.174|                  1.0|                   4.0|\n",
      "|2024-02-14 00:00:00|    0.169|                 -1.0|                   5.0|\n",
      "|2024-02-26 00:00:00|   -0.121|                  0.0|                   5.0|\n",
      "|2024-02-27 00:00:00|    0.062|                  0.0|                   9.0|\n",
      "|2024-02-28 00:00:00|    0.148|                 -1.0|                  -6.0|\n",
      "|2024-03-05 00:00:00|    0.019|                 -1.0|                   4.0|\n",
      "|2024-03-06 00:00:00|    0.081|                  1.0|                   1.0|\n",
      "|2024-03-07 00:00:00|    0.159|                  0.0|                   4.0|\n",
      "|2024-03-11 00:00:00|    0.128|                  1.0|                  -1.0|\n",
      "|2024-03-12 00:00:00|    0.233|                  0.0|                   2.0|\n",
      "|2024-03-13 00:00:00|    0.323|                  0.0|                  -2.0|\n",
      "+-------------------+---------+---------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "  DATE,\n",
    "  ROUND(sentiment, 3) AS sentiment,\n",
    "  LEAD(ROUND(((s_close - s_open)/s_open)*100), 1) OVER (ORDER BY DATE) AS next_day_stock_return,\n",
    "  LEAD(ROUND(((c_close - c_open)/c_open)*100), 1) OVER (ORDER BY DATE) AS next_day_crypto_return\n",
    "FROM transformed_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57bb087-eac4-486d-a09a-d6b247d1de79",
   "metadata": {},
   "source": [
    "#### 8.7 Sentiment Leading to Breakouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6664a36c-cb14-40ce-905b-d0eb6529df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 17:31:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------------------+---------------------+\n",
      "|               DATE|         sentiment|next_day_crypto_return|next_day_stock_return|\n",
      "+-------------------+------------------+----------------------+---------------------+\n",
      "|2024-03-13 00:00:00|0.3234941020785715|                  NULL|                 NULL|\n",
      "+-------------------+------------------+----------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/10 17:31:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Big Next-Day Jumps After Sentiment Spikes\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "   DATE,\n",
    "   sentiment,\n",
    "   LEAD(((c_close - c_open)/c_open)*100, 1) OVER (ORDER BY DATE) AS next_day_crypto_return,\n",
    "   LEAD(((s_close - s_open)/s_open)*100, 1) OVER (ORDER BY DATE) AS next_day_stock_return\n",
    "FROM transformed_data\n",
    "WHERE sentiment > 0.3\n",
    "ORDER BY DATE\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d850a5-3d0a-4522-9ea3-5a0d1304685f",
   "metadata": {},
   "source": [
    "#### 8.8 Big Move Days with Sentiment Neutrality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39674e4c-06c7-49cd-8932-c6afd01dd1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+-----------+\n",
      "|DATE|sentiment|stock_move|crypto_move|\n",
      "+----+---------+----------+-----------+\n",
      "+----+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Detect Market Movers Without Sentiment Spike\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "  DATE,\n",
    "  sentiment,\n",
    "  ((s_close - s_open)/s_open)*100 AS stock_move,\n",
    "  ((c_close - c_open)/c_open)*100 AS crypto_move\n",
    "FROM transformed_data\n",
    "WHERE ABS(sentiment) < 0.05 AND (ABS((s_close - s_open)/s_open) > 3 OR ABS((c_close - c_open)/c_open) > 5)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150fec0-21cf-4d8d-9d0f-42ef7ff18084",
   "metadata": {},
   "source": [
    "**Insights from Combined Analysis**\n",
    "\n",
    "\n",
    "**8.1 Price Comparison Between Stocks & Crypto**\n",
    "- Crypto markets show **greater daily volatility** compared to stock markets.\n",
    "- Even on relatively flat stock days, crypto often exhibits **larger swings**, reinforcing the high-risk/high-reward profile of crypto assets.\n",
    "\n",
    "**8.2 Sentiment vs Daily Returns**\n",
    "- There appears to be a **moderate positive correlation** between sentiment polarity and market returns.\n",
    "- However, the relationship is **not always consistent**, especially for stocks.\n",
    "\n",
    "**8.3 High Sentiment but Prices Dropped**\n",
    "\n",
    "- On **2024-03-13**, sentiment was high (0.323), yet stock prices **fell**, and crypto rose modestly.\n",
    "- This shows **optimistic sentiment does not always lead to market gains**, possibly due to overhype or delayed reactions.\n",
    "\n",
    "**8.4 Low Sentiment but Prices Rose**\n",
    "\n",
    "- Several dates (e.g., 2024-02-01, 2024-02-26) saw **negative or neutral sentiment but strong market gains**.\n",
    "- Suggests **market rebounds or hidden drivers** that sentiment analysis may miss.\n",
    "\n",
    "**8.5 Weekly Avg Sentiment & Market Return**\n",
    "\n",
    "- Some weeks with **positive sentiment** align with higher returns (Week 6, 7, 46).\n",
    "- But some weeks show **positive sentiment with negative returns**, reinforcing that **sentiment alone isn't sufficient**.\n",
    "\n",
    "**8.6 Lagged Sentiment vs Future Returns**\n",
    "\n",
    "- No clear or strong predictive power of sentiment on **next-day returns**.\n",
    "- Some high-sentiment days are followed by losses (e.g., crypto -5%), suggesting markets may **already price in sentiment**.\n",
    "\n",
    "**8.7 Sentiment Spikes Leading to Breakouts**\n",
    "\n",
    "- Limited evidence that **sentiment spikes lead to breakouts**.\n",
    "- In fact, after high sentiment days, next-day returns were either missing or inconsistent â€” possibly due to **data gaps** or **weak lag correlation**.\n",
    "\n",
    "**8.8 Big Move Days with Sentiment Neutrality**\n",
    "\n",
    "- No results found.\n",
    "- Suggests that **major market moves are typically accompanied by stronger sentiment**, reinforcing the link between news and volatility.\n",
    "\n",
    "**Final Conclusion**\n",
    "\n",
    "The analysis shows:\n",
    "- **Crypto markets** are more volatile than stocks.\n",
    "- **Sentiment is partially correlated** with market returns â€” especially crypto â€” but **not reliably predictive**.\n",
    "- There are **exceptions** where sentiment and price movement diverge.\n",
    "- Sentiment analysis adds **context but not certainty** to market prediction â€” combining it with technical/volume data may improve models.\n",
    "\n",
    "This end-to-end querying builds a **360-degree view** of how public mood interacts with price behavior across asset classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e6a8e-8939-4633-baec-6e3c6867e356",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b37b65-90b0-4841-8882-575241f78134",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Notes:</strong>\n",
    "    Combined with Transformations for clarity. Setup details can be found under 'Setting up S3 for Loading', while uploads are documented in sections 2.5, 3.1, 4.1, 8.2, 8.3, and 8.4, each titled with the prefix 'Loading - '.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d41f0-b166-475b-a8bf-7b6bd79e55ce",
   "metadata": {},
   "source": [
    "## Querying the Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3565953-88ad-4a78-92fe-e4e3937623c8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px; border-radius: 5px;\">\n",
    "    <strong>Notes:</strong>\n",
    "    Thanks to the comprehensive work completed during the ETL phases, queries can now be executed with ease, offering low latency and improved clarity, as demonstrated below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0456065f-7a15-427c-a17f-94ffaace1c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb\n",
      "  Using cached duckdb-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (966 bytes)\n",
      "Using cached duckdb-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.2 MB)\n",
      "Installing collected packages: duckdb\n",
      "Successfully installed duckdb-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202ee02a-4536-45ca-b8f6-9a82516c0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e07ed8-56bf-4739-8754-0fb0a01a1e30",
   "metadata": {},
   "source": [
    "#### Querying - 2.5 Volume Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eda20b2-9855-41b5-abfb-06dfaf2a5132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset</th>\n",
       "      <th>date</th>\n",
       "      <th>volume_spike</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRX</td>\n",
       "      <td>2024-12-03</td>\n",
       "      <td>15.97</td>\n",
       "      <td>Crypto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRX</td>\n",
       "      <td>2024-08-20</td>\n",
       "      <td>8.53</td>\n",
       "      <td>Crypto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ETH</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>7.38</td>\n",
       "      <td>Crypto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LINK</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>6.71</td>\n",
       "      <td>Crypto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVAX</td>\n",
       "      <td>2024-06-22</td>\n",
       "      <td>6.28</td>\n",
       "      <td>Crypto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  asset       date  volume_spike    type\n",
       "0   TRX 2024-12-03         15.97  Crypto\n",
       "1   TRX 2024-08-20          8.53  Crypto\n",
       "2   ETH 2024-08-05          7.38  Crypto\n",
       "3  LINK 2024-08-05          6.71  Crypto\n",
       "4  AVAX 2024-06-22          6.28  Crypto"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set S3 credentials (skip if public bucket)\n",
    "duckdb.sql(f\"\"\"\n",
    "SET s3_region='{os.environ[\"AWS_REGION\"]}';\n",
    "SET s3_access_key_id='{os.environ[\"AWS_ACCESS_KEY_ID\"]}';\n",
    "SET s3_secret_access_key='{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}';\n",
    "\"\"\")\n",
    "\n",
    "# Query directly from S3\n",
    "df = duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM 's3://ucl-de/data/crypto_top_volume_spikes.parquet'\n",
    "\"\"\").df()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421df91-afee-42c1-87e2-578a83c7628a",
   "metadata": {},
   "source": [
    "#### Querying - 3.1 Daily Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e2c97c-ca2d-4b17-a47e-13cb22e6f161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coin_name</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>-0.0293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>-0.0795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>0.0237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>-0.0493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>-0.0354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  coin_name       date  daily_return\n",
       "0       ADA 2024-01-02       -0.0293\n",
       "1       ADA 2024-01-03       -0.0795\n",
       "2       ADA 2024-01-04        0.0237\n",
       "3       ADA 2024-01-05       -0.0493\n",
       "4       ADA 2024-01-06       -0.0354"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set S3 credentials (skip if public bucket)\n",
    "duckdb.sql(f\"\"\"\n",
    "SET s3_region='{os.environ[\"AWS_REGION\"]}';\n",
    "SET s3_access_key_id='{os.environ[\"AWS_ACCESS_KEY_ID\"]}';\n",
    "SET s3_secret_access_key='{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}';\n",
    "\"\"\")\n",
    "\n",
    "# Query directly from S3\n",
    "df = duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM 's3://ucl-de/data/crypto_daily_return.parquet'\n",
    "\"\"\").df()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693bfba6-0137-464d-858c-5c4d11db89fc",
   "metadata": {},
   "source": [
    "#### Querying - 4.1 Rolling 7-day volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63547204-7850-46af-a821-7bae2a55c087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coin_name</th>\n",
       "      <th>date</th>\n",
       "      <th>rolling_volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>0.0354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>0.0516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>0.0434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>0.0376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADA</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>0.0346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  coin_name       date  rolling_volatility\n",
       "0       ADA 2024-01-03              0.0354\n",
       "1       ADA 2024-01-04              0.0516\n",
       "2       ADA 2024-01-05              0.0434\n",
       "3       ADA 2024-01-06              0.0376\n",
       "4       ADA 2024-01-07              0.0346"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set S3 credentials (skip if public bucket)\n",
    "duckdb.sql(f\"\"\"\n",
    "SET s3_region='{os.environ[\"AWS_REGION\"]}';\n",
    "SET s3_access_key_id='{os.environ[\"AWS_ACCESS_KEY_ID\"]}';\n",
    "SET s3_secret_access_key='{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}';\n",
    "\"\"\")\n",
    "\n",
    "# Query directly from S3\n",
    "df = duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM 's3://ucl-de/data/crypto_volatility.parquet'\n",
    "\"\"\").df()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db87f5-58e8-4d40-b24e-ad32262a352d",
   "metadata": {},
   "source": [
    "#### Querying - 8.2 Sentiment vs Daily Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d7be4ba-5085-4ff3-8143-5408cba96b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>stock_pct_change</th>\n",
       "      <th>crypto_pct_change</th>\n",
       "      <th>avg_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-4.78</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-25</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-26</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  stock_pct_change  crypto_pct_change  avg_sentiment\n",
       "0 2024-01-02             -0.32               1.69          0.170\n",
       "1 2024-01-03             -0.03              -4.78          0.179\n",
       "2 2024-01-24              0.00               0.44          0.125\n",
       "3 2024-01-25             -0.14              -0.37          0.204\n",
       "4 2024-01-26              0.03               4.57          0.064"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set S3 credentials (skip if public bucket)\n",
    "duckdb.sql(f\"\"\"\n",
    "SET s3_region='{os.environ[\"AWS_REGION\"]}';\n",
    "SET s3_access_key_id='{os.environ[\"AWS_ACCESS_KEY_ID\"]}';\n",
    "SET s3_secret_access_key='{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}';\n",
    "\"\"\")\n",
    "\n",
    "# Query directly from S3\n",
    "df = duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM 's3://ucl-de/data/sentiment_vs_daily_return.parquet'\n",
    "\"\"\").df()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d307ef-1106-406e-b85e-0cafe6187775",
   "metadata": {},
   "source": [
    "#### Querying - 8.3 Days When Sentiment Was High but Prices Dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05abb847-f955-46b1-bab7-1a8fec849d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stock_pct_change</th>\n",
       "      <th>crypto_pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-13</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  sentiment  stock_pct_change  crypto_pct_change\n",
       "0 2024-03-13      0.323             -0.24               2.27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set S3 credentials (skip if public bucket)\n",
    "duckdb.sql(f\"\"\"\n",
    "SET s3_region='{os.environ[\"AWS_REGION\"]}';\n",
    "SET s3_access_key_id='{os.environ[\"AWS_ACCESS_KEY_ID\"]}';\n",
    "SET s3_secret_access_key='{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}';\n",
    "\"\"\")\n",
    "\n",
    "# Query directly from S3\n",
    "df = duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM 's3://ucl-de/data/sentiment_high_prices_low.parquet'\n",
    "\"\"\").df()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9409cc-7ed6-4d97-b959-832644ca717c",
   "metadata": {},
   "source": [
    "#### Querying - 8.4 Days When Sentiment Was Low but Prices Rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d28ab111-112c-4417-aff4-b1a2a369e502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stock_pct_change</th>\n",
       "      <th>crypto_pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-16</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  sentiment  stock_pct_change  crypto_pct_change\n",
       "0 2024-02-01     -0.028              0.63               1.22\n",
       "1 2024-02-26     -0.121             -0.90               5.22\n",
       "2 2024-12-16      0.025              1.22               1.62"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set S3 credentials (skip if public bucket)\n",
    "duckdb.sql(f\"\"\"\n",
    "SET s3_region='{os.environ[\"AWS_REGION\"]}';\n",
    "SET s3_access_key_id='{os.environ[\"AWS_ACCESS_KEY_ID\"]}';\n",
    "SET s3_secret_access_key='{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}';\n",
    "\"\"\")\n",
    "\n",
    "# Query directly from S3\n",
    "df = duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM 's3://ucl-de/data/sentiment_low_prices_high.parquet'\n",
    "\"\"\").df()\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
